{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ipynb.fs.full.Random_Sample_Mapper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = os.path.abspath(\"BPR_OPT_Binary_Model.ipynb\")\n",
    "users_items_file_path = os.path.join(os.path.dirname(notebook_path), \"data/australian_users_items.json\")\n",
    "items_file_path = os.path.join(os.path.dirname(notebook_path), \"data/items_meta_data.json\")\n",
    "users_meta_data_file_path = os.path.join(os.path.dirname(notebook_path), \"data/users_meta_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_items = []\n",
    "with open(users_items_file_path, 'r') as data:\n",
    "    for line in data:\n",
    "        users_items.append(ast.literal_eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(items_file_path, 'r') as data:\n",
    "    games_dict = json.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(users_meta_data_file_path, 'r') as file:\n",
    "    users_meta_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "playtimesPerItem = defaultdict(dict)\n",
    "playtimesPerUser = defaultdict(dict)\n",
    "itemNames = defaultdict(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_usersPerItem = defaultdict(set)\n",
    "train_itemsPerUser = defaultdict(set)\n",
    "test_usersPerItem = defaultdict(set)\n",
    "test_itemsPerUser = defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for game in games_dict:\n",
    "    if 'owners' in games_dict[game]:\n",
    "        usersPerItem[game] = set(games_dict[game]['owners'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users_items:\n",
    "    u_id = user['user_id']\n",
    "    items = [item['item_id'] for item in user['items']]\n",
    "    itemsPerUser[u_id] = items\n",
    "    \n",
    "    items_train, items_test = train_test_split(items, test_size = 0.995)\n",
    "    train_itemsPerUser[u_id] = items_train\n",
    "    test_itemsPerUser[u_id] = items_test\n",
    "\n",
    "    playtimesPerUser[user['user_id']] = dict((item['item_id'], item['playtime_forever']) for item in user['items'])\n",
    "    for item in user['items']:\n",
    "        itemNames[item['item_id']] = item['item_name']\n",
    "        playtimesPerItem[item['item_id']][user['user_id']] = item['playtime_forever']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in train_itemsPerUser:\n",
    "    for item in train_itemsPerUser[user]:\n",
    "        train_usersPerItem[item].add(user)\n",
    "        \n",
    "    for item in test_itemsPerUser[user]:\n",
    "        test_usersPerItem[item].add(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nUsers = len(train_itemsPerUser)\n",
    "train_nItems = len(train_usersPerItem)\n",
    "train_users = list(train_itemsPerUser.keys())\n",
    "train_items = list(train_usersPerItem.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_counts = dict((k, len(v)) for k, v in train_itemsPerUser.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = 'data/sample_in.tsv'\n",
    "mapout1 = 'data/sample_map1.tsv'\n",
    "mapout2 = 'data/sample_map2.tsv'\n",
    "outfile = 'data/sample_out.tsv'\n",
    "\n",
    "f = open(datafile,'w')\n",
    "for user, items in train_itemsPerUser.items():\n",
    "    for item in items:\n",
    "        print(default_formatter(user,item), file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run two stages of mapreduce\n",
    "mapper1 = Mapper(user_item_counts, oversampling=10)\n",
    "mapreduce(datafile, mapout1, mapper=mapper1, reducer=reducer)\n",
    "mapreduce(datafile, mapout2, mapper=indicator_mapper)  # map the data again\n",
    "mapreduce([mapout1, mapout2], outfile, reducer=indicator_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExternalSchedule(object):\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def generate_samples(self, data):\n",
    "        f = open(self.filepath)\n",
    "        samples = [map(str, line.strip().split()) for line in f]\n",
    "        random.shuffle(samples)  # important!\n",
    "        for u, i, j in samples:\n",
    "            yield u, i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = ExternalSchedule(outfile)  # schedule is one-indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Latent Factor Model with Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple non-biased latent factor model that is wrapped into a binary function (sigmoid function) as a base line model, using popularity as the item's sole feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemBiases = defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each user and item we now have a low dimensional descriptor (representing that user's preferences, and that item's properties), of dimension K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "userGamma = {}\n",
    "itemGamma = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in train_itemsPerUser:\n",
    "    userGamma[u] = [random.random() * 0.1 - 0.05 for k in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_usersPerItem:\n",
    "    itemGamma[i] = [random.random() * 0.1 - 0.05 for k in range(K)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use another library in this example to perform gradient descent. This library requires that we pass it a \"flat\" parameter vector (theta) containing all of our parameters. This utility function just converts between a flat feature vector, and our model parameters, i.e., it \"unpacks\" theta into our offset and bias parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global itemBiases\n",
    "    global userGamma\n",
    "    global itemGamma\n",
    "    index = 0\n",
    "    itemBiases = dict(zip(train_items, theta[0:index + train_nItems]))\n",
    "    index += train_nItems\n",
    "    for u in train_users:\n",
    "        userGamma[u] = theta[index:index + K]\n",
    "        index += K\n",
    "    for i in train_items:\n",
    "        itemGamma[i] = theta[index:index + K]\n",
    "        index += K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner(x, y):\n",
    "    return sum([a*b for a,b in zip(x,y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    #Numerically stable sigmoid function.\n",
    "    #Taken from: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    if x >= 0:\n",
    "        z = np.exp(-x)\n",
    "        return 1 / (1 + z)\n",
    "    else:\n",
    "        # if x is less than zero then z will be small, denom can't be\n",
    "        # zero because it's 1+z.\n",
    "        z = np.exp(x)\n",
    "        return z / (1 + z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "f(u, i, j) = \\gamma_u \\gamma_i + \\beta_i - (\\gamma_u \\gamma_j + \\beta_j)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "p(i >_u j) = \\sigma(f(u, i, j))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(user, item_i, item_j):\n",
    "    return inner(userGamma[user], itemGamma[item_i]) + itemBiases[item_i] - (inner(userGamma[user], itemGamma[item_j]) + itemBiases[item_j]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\text{BPR-OPT} := \\text{argmax} \\ln(\\sigma(\\gamma_u \\gamma_i + \\beta_i - (\\gamma_u \\gamma_j + \\beta_j)))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\text{Cost Function}:= \\sum_{u,i,j} -ln\\left( 1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)} \\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(u, i, j):\n",
    "    return u[1:len(u)-1], i[2:len(i)-2], j[1:len(j)-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta):\n",
    "    unpack(theta)\n",
    "    cost = 0\n",
    "    for u, i, j in sampler.generate_samples(data):\n",
    "        u, i, j = trim(u, i ,j)\n",
    "        x = prediction(u, i, j)\n",
    "\n",
    "        cost += np.log(sigmoid(x))\n",
    "        \n",
    "    print(-cost)\n",
    "        \n",
    "    return -cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\gamma_{u,k}} = \\frac{(\\gamma_{i,k} - \\gamma_{j,k}) \\cdot e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}{1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\gamma_{i,k}} = \\frac{\\gamma_{u,k} \\cdot e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}{1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\gamma_{j,k}} = -\\frac{\\gamma_{u,k} \\cdot e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}{1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\beta_i} = \\frac{e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}{1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\beta_j} = -\\frac{e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}{1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(theta):\n",
    "    unpack(theta)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    dUserGamma = {}\n",
    "    dItemGamma = {}\n",
    "    for u in train_users:\n",
    "        dUserGamma[u] = [0.0 for k in range(K)]\n",
    "    for i in train_items:\n",
    "        dItemGamma[i] = [0.0 for k in range(K)]\n",
    "    for u, i, j in sampler.generate_samples(data):\n",
    "        u, i, j = trim(u, i ,j)\n",
    "        x = prediction(u, i ,j)\n",
    "        dbase = np.exp(-x) / (1 + np.exp(-x))\n",
    "        dItemBiases[i] += dbase\n",
    "        dItemBiases[j] += -dbase\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += (itemGamma[i][k] - itemGamma[j][k]) * dbase\n",
    "            dItemGamma_k = userGamma[u][k] * dbase\n",
    "            dItemGamma[i][k] += dItemGamma_k\n",
    "            dItemGamma[j][k] += -dItemGamma_k\n",
    "    dtheta = [dItemBiases[i] for i in train_items]\n",
    "    for u in train_users:\n",
    "        dtheta += dUserGamma[u]\n",
    "    for i in train_items:\n",
    "        dtheta += dItemGamma[i]\n",
    "    return np.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20112.32888593956\n",
      "20210.48345567925\n",
      "20120.56756774348\n",
      "20113.078795422894\n",
      "20112.397641205458\n",
      "20112.335193930237\n",
      "20112.32946470475\n",
      "20112.328939042058\n",
      "20112.32889081179\n",
      "20112.328886386567\n",
      "20112.328885980463\n",
      "20112.32888594329\n",
      "20112.328885939984\n",
      "20112.328885939518\n",
      "20112.328885939583\n",
      "20112.32888593951\n",
      "20112.32888593943\n",
      "20112.328885939405\n",
      "20112.32888593957\n",
      "20112.328885939503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-9.42986008e-15, -4.73319383e-15,  4.72190682e-15, ...,\n",
       "         1.25060359e-02,  3.34352364e-02,  1.97081035e-02]),\n",
       " 20112.328885939503,\n",
       " {'grad': array([ 1.99856945,  1.00315556, -1.00076339, ..., -0.02134534,\n",
       "          0.02569994,  0.07383677]),\n",
       "  'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH',\n",
       "  'funcalls': 20,\n",
       "  'nit': 1,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.optimize.fmin_l_bfgs_b(cost, [0.0]*train_nItems + # Initialize beta\n",
    "                                [random.random() * 0.1 - 0.05 for k in range(K*(train_nUsers + train_nItems))], # Gamma\n",
    "                             derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
