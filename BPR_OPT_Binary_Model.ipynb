{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import copy \n",
    "import itertools\n",
    "from itertools import islice\n",
    "from tqdm import *\n",
    "from math import ceil\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from scipy.sparse import csr_matrix, dok_matrix\n",
    "from ipynb.fs.full.Random_Sample_Mapper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notebook_path = os.path.abspath(\"BPR_OPT_Binary_Model.ipynb\")\n",
    "users_items_file_path = os.path.join(os.path.dirname(notebook_path), \"data/australian_users_items.json\")\n",
    "items_file_path = os.path.join(os.path.dirname(notebook_path), \"data/items_meta_data.json\")\n",
    "users_meta_data_file_path = os.path.join(os.path.dirname(notebook_path), \"data/users_meta_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_items = []\n",
    "with open(users_items_file_path, 'r') as data:\n",
    "    for line in data:\n",
    "        users_items.append(ast.literal_eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(items_file_path, 'r') as data:\n",
    "    games_dict = json.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(users_meta_data_file_path, 'r') as file:\n",
    "    users_meta_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using default dict for efficient data retrieval for users-items playtime relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "playtimesPerItem = defaultdict(dict)\n",
    "playtimesPerUser = defaultdict(dict)\n",
    "itemNames = defaultdict(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_usersPerItem = defaultdict(set)\n",
    "train_itemsPerUser = defaultdict(set)\n",
    "test_usersPerItem = defaultdict(set)\n",
    "test_itemsPerUser = defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for game in games_dict:\n",
    "    if 'owners' in games_dict[game]:\n",
    "        usersPerItem[game] = set(games_dict[game]['owners'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for user in users_items:\n",
    "    u_id = user['user_id']\n",
    "    items = [item['item_id'] for item in user['items']]\n",
    "    itemsPerUser[u_id] = items\n",
    "    \n",
    "    items_train, items_test = train_test_split(items, test_size = 0.20)\n",
    "    train_itemsPerUser[u_id] = items_train\n",
    "    test_itemsPerUser[u_id] = items_test\n",
    "\n",
    "    playtimesPerUser[user['user_id']] = dict((item['item_id'], item['playtime_forever']) for item in user['items'])\n",
    "    for item in user['items']:\n",
    "        itemNames[item['item_id']] = item['item_name']\n",
    "        playtimesPerItem[item['item_id']][user['user_id']] = item['playtime_forever']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for user in train_itemsPerUser:\n",
    "    for item in train_itemsPerUser[user]:\n",
    "        train_usersPerItem[item].add(user)\n",
    "        \n",
    "    for item in test_itemsPerUser[user]:\n",
    "        test_usersPerItem[item].add(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Scheduled Sampling with Map-Reduce Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nUsers = len(itemsPerUser)\n",
    "nItems = len(usersPerItem)\n",
    "users = list(itemsPerUser.keys())\n",
    "items = list(usersPerItem.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_user_item_counts = dict((k, len(v)) for k, v in train_itemsPerUser.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_user_item_counts = dict((k, len(v)) for k, v in test_itemsPerUser.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_datafile = 'data/train_sample_in.tsv'\n",
    "train_mapout1 = 'data/train_sample_map1.tsv'\n",
    "train_mapout2 = 'data/train_sample_map2.tsv'\n",
    "train_outfile = 'data/train_sample_out.tsv'\n",
    "\n",
    "f = open(train_datafile,'w')\n",
    "for u, its in train_itemsPerUser.items():\n",
    "    for i in its:\n",
    "        print(default_formatter(u,i), file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run two stages of mapreduce\n",
    "train_mapper = Mapper(train_user_item_counts)\n",
    "mapreduce(train_datafile, train_mapout1, mapper=train_mapper, reducer=reducer)\n",
    "mapreduce(train_datafile, train_mapout2, mapper=indicator_mapper)  # map the data again\n",
    "mapreduce([train_mapout1, train_mapout2], train_outfile, reducer=indicator_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_sample():\n",
    "    test_sample = []\n",
    "    for u, its in test_itemsPerUser.items():\n",
    "        for i in its:\n",
    "            j = random.choice(items)\n",
    "            test_sample.append((u, i, j))\n",
    "    return test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trim(u, i, j):\n",
    "    return u[1:len(u)-1], i[2:len(i)-2], j[1:len(j)-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExternalSchedule(object):\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        \n",
    "    def generate_random_samples(self, size=None, rand=True):\n",
    "        f = open(self.filepath)\n",
    "        samples = [map(str, line.strip().split()) for line in f]\n",
    "        if rand:\n",
    "            random.shuffle(samples)  # important!\n",
    "        if size is None:\n",
    "            size = len(samples)\n",
    "        for u, i, j in samples[:size]:\n",
    "            yield u, i, j\n",
    "            \n",
    "    def generate_list_samples(self):\n",
    "        f = open(self.filepath)\n",
    "        samples = [map(str, line.strip().split()) for line in f]\n",
    "        triplets = []\n",
    "        for u, i, j in samples:\n",
    "            u, i, j = trim(u, i, j)\n",
    "            triplets.append((u, i, j))\n",
    "            \n",
    "        return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampler = ExternalSchedule(train_outfile)  # schedule is one-indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sample = sampler.generate_list_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_sample) * 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inner(x, y):\n",
    "    return sum([a*b for a,b in zip(x,y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_label(u, i, j):\n",
    "    c = Counter(itemsPerUser[u])\n",
    "    if c[i] > c[j]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_outputs(sample):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for u, i, j in sample:\n",
    "        predict = sigmoid(prediction(u, i, j))\n",
    "        label = binary_label(u, i, j)\n",
    "        predictions.append(predict)\n",
    "        labels.append(label)\n",
    "            \n",
    "    return np.rint(predictions), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    differences = [1 if x == y else 0 for x, y in zip(predictions, labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    #Numerically stable sigmoid function.\n",
    "    #Taken from: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    if x >= 0:\n",
    "        z = np.exp(-x)\n",
    "        return 1 / (1 + z)\n",
    "    else:\n",
    "        # if x is less than zero then z will be small, denom can't be\n",
    "        # zero because it's 1+z.\n",
    "        z = np.exp(x)\n",
    "        return z / (1 + z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple (Biase Only) Latent Factor Model with Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itemBiases = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global itemBiases\n",
    "    itemBiases = dict(zip(items, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "f(i, j) = \\beta_i - \\beta_j\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "p(i >_u j) = \\sigma(f(i, j))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(u, item_i, item_j):\n",
    "    return itemBiases[item_i] - itemBiases[item_j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\text{BPR-OPT} := \\text{argmax} \\ln(\\sigma(\\beta_i - \\beta_j))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\text{Cost Function}:= \\sum_{u,i,j} ln\\left( \\frac{1}{1 + e^{\\beta_j - \\beta_i}} \\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(theta):\n",
    "    unpack(theta)\n",
    "    cost = 0.0\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for u, i, j in sampler.generate_random_samples(size=70000):\n",
    "        u, i, j = trim(u, i ,j)\n",
    "        x = prediction(u, i, j)\n",
    "        \n",
    "        predictions.append(sigmoid(x))\n",
    "        labels.append(binary_label(u, i, j))\n",
    "\n",
    "        cost += np.log(sigmoid(x))\n",
    "        \n",
    "    print('Current Cost: %s' % -cost)\n",
    "    print('Current Accuracy: %s' % accuracy(np.rint(predictions), labels))\n",
    "    print('-------------------------------------------------------------------')\n",
    "    \n",
    "    return -cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial }{\\partial x} ln\\sigma(x) = \\frac{1}{1 + e^x} = \\sigma(-x)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\beta_i} = \\frac{e^{\\beta_j - \\beta_i}}{1 + e^{\\beta_j - \\beta_i}} = \\frac{1}{1 + e^{\\beta_i - \\beta_j}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\beta_j} = -\\frac{e^{\\beta_j - \\beta_i}}{1 + e^{\\beta_j - \\beta_i}} = -\\frac{1}{1 + e^{\\beta_i - \\beta_j}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def derivative(theta):\n",
    "    unpack(theta)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    for u, i, j in sampler.generate_random_samples(size=70000):\n",
    "        u, i, j = trim(u, i ,j)\n",
    "        x = prediction(u, i, j)\n",
    "        dbase = -1 / (1 + np.exp(x)) # negative gradient descent for maximizing\n",
    "        dItemBiases[i] += dbase\n",
    "        dItemBiases[j] -= dbase\n",
    "    dtheta = [dItemBiases[i] for i in items]\n",
    "    return np.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res, f, d = sy.optimize.fmin_l_bfgs_b(cost, [0.0]*nItems, derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unpack(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sample = generate_test_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions, labels = generate_outputs(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Latent Factor Model with Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple non-biased latent factor model that is wrapped into a binary function (sigmoid function) as a base line model, using popularity as the item's sole feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itemBiases = defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each user and item we now have a low dimensional descriptor (representing that user's preferences, and that item's properties), of dimension K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userGamma = {}\n",
    "itemGamma = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for u in itemsPerUser:\n",
    "    userGamma[u] = [random.random() * 0.1 - 0.05 for k in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in usersPerItem:\n",
    "    itemGamma[i] = [random.random() * 0.1 - 0.05 for k in range(K)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use another library in this example to perform gradient descent. This library requires that we pass it a \"flat\" parameter vector (theta) containing all of our parameters. This utility function just converts between a flat feature vector, and our model parameters, i.e., it \"unpacks\" theta into our offset and bias parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global itemBiases\n",
    "    global userGamma\n",
    "    global itemGamma\n",
    "    index = 0\n",
    "    itemBiases = dict(zip(items, theta[0:index + nItems]))\n",
    "    index += nItems\n",
    "    for u in users:\n",
    "        userGamma[u] = theta[index:index + K]\n",
    "        index += K\n",
    "    for i in items:\n",
    "        itemGamma[i] = theta[index:index + K]\n",
    "        index += K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "f(u, i, j) = \\gamma_u \\gamma_i + \\beta_i - (\\gamma_u \\gamma_j + \\beta_j)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "p(i >_u j) = \\sigma(f(u, i, j))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(user, item_i, item_j):\n",
    "    return inner(userGamma[user], itemGamma[item_i]) + itemBiases[item_i] - (inner(userGamma[user], itemGamma[item_j]) + itemBiases[item_j]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\text{BPR-OPT} := \\text{argmax} \\ln(\\sigma(\\gamma_u \\gamma_i + \\beta_i - (\\gamma_u \\gamma_j + \\beta_j)))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\text{Cost Function}:= \\sum_{u,i,j} ln\\left( \\frac{1}{1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}} \\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(theta):\n",
    "    unpack(theta)\n",
    "    cost = 0.0\n",
    "    predictions = []\n",
    "    for u, i, j in sampler.generate_random_samples(size=5000, rand=False):\n",
    "        u, i, j = trim(u, i ,j)\n",
    "        x = prediction(u, i, j)\n",
    "        \n",
    "        predictions.append(sigmoid(x))\n",
    "        labels.append(binary_label(u, i, j))\n",
    "\n",
    "        cost += np.log(sigmoid(x))\n",
    "        \n",
    "    print('Current Cost: %s' % -cost)\n",
    "    print('Current Accuracy: %s' % accuracy(predictions, labels))\n",
    "    print('-------------------------------------------------------------------')\n",
    "        \n",
    "    return -cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\gamma_{u,k}} = \\frac{(\\gamma_{i,k} - \\gamma_{j,k}) \\cdot e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}{1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\gamma_{i,k}} = \\frac{\\gamma_{u,k} \\cdot e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}{1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\gamma_{j,k}} = -\\frac{\\gamma_{u,k} \\cdot e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}{1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\beta_i} = \\frac{e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}{1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\beta_j} = -\\frac{e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}{1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def derivative(theta):\n",
    "    unpack(theta)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    dUserGamma = {}\n",
    "    dItemGamma = {}\n",
    "    for u in users:\n",
    "        dUserGamma[u] = [0.0 for k in range(K)]\n",
    "    for i in items:\n",
    "        dItemGamma[i] = [0.0 for k in range(K)]\n",
    "    for u, i, j in sampler.generate_random_samples(size=5000, rand=False):\n",
    "        u, i, j = trim(u, i ,j)\n",
    "        x = prediction(u, i ,j)\n",
    "        dbase = -1 / (1 + np.exp(x)) # negative gradient descent for maximizing\n",
    "        dItemBiases[i] += dbase\n",
    "        dItemBiases[j] -= dbase\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += (itemGamma[i][k] - itemGamma[j][k]) * dbase\n",
    "            dItemGamma_k = userGamma[u][k] * dbase\n",
    "            dItemGamma[i][k] += dItemGamma_k\n",
    "            dItemGamma[j][k] -= dItemGamma_k\n",
    "    dtheta = [dItemBiases[i] for i in items]\n",
    "    for u in users:\n",
    "        dtheta += dUserGamma[u]\n",
    "    for i in items:\n",
    "        dtheta += dItemGamma[i]\n",
    "    return np.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_res, complete_f, complete_d = sy.optimize.fmin_l_bfgs_b(cost, \n",
    "                                [0.0]*nItems + # Initialize beta\n",
    "                                [random.random() * 0.1 - 0.05 for k in range(K*(nUsers + nItems))], # Gamma\n",
    "                                derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unpack(complete_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions, labels = generate_outputs(train_sample[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels = generate_outputs(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
