{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import copy \n",
    "import itertools\n",
    "from itertools import islice\n",
    "from tqdm import *\n",
    "from math import ceil\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from scipy.sparse import csr_matrix, dok_matrix\n",
    "from ipynb.fs.full.Random_Sample_Mapper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notebook_path = os.path.abspath(\"BPR_OPT_Binary_Model.ipynb\")\n",
    "users_items_file_path = os.path.join(os.path.dirname(notebook_path), \"data/australian_users_items.json\")\n",
    "items_file_path = os.path.join(os.path.dirname(notebook_path), \"data/items_meta_data.json\")\n",
    "users_meta_data_file_path = os.path.join(os.path.dirname(notebook_path), \"data/users_meta_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_items = []\n",
    "with open(users_items_file_path, 'r') as data:\n",
    "    for line in data:\n",
    "        users_items.append(ast.literal_eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(items_file_path, 'r') as data:\n",
    "    games_dict = json.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(users_meta_data_file_path, 'r') as file:\n",
    "    users_meta_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using default dict for efficient data retrieval for users-items playtime relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "playtimesPerItem = defaultdict(dict)\n",
    "playtimesPerUser = defaultdict(dict)\n",
    "itemNames = defaultdict(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for game in games_dict:\n",
    "    if 'owners' in games_dict[game]:\n",
    "        usersPerItem[game] = set(games_dict[game]['owners'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for user in users_items:\n",
    "    u_id = user['user_id']\n",
    "    items = [item['item_id'] for item in user['items']]\n",
    "    itemsPerUser[u_id] = items\n",
    "    playtimesPerUser[user['user_id']] = dict((item['item_id'], item['playtime_forever']) for item in user['items'])\n",
    "    for item in user['items']:\n",
    "        itemNames[item['item_id']] = item['item_name']\n",
    "        playtimesPerItem[item['item_id']][user['user_id']] = item['playtime_forever']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduled Sampling with Map-Reduce Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nUsers = len(itemsPerUser)\n",
    "nItems = len(usersPerItem)\n",
    "users = list(itemsPerUser.keys())\n",
    "items = list(usersPerItem.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_item_counts = dict((k, len(v)) for k, v in itemsPerUser.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = 'data/sample_in.tsv'\n",
    "mapout1 = 'data/sample_map1.tsv'\n",
    "mapout2 = 'data/sample_map2.tsv'\n",
    "outfile = 'data/sample_out.tsv'\n",
    "\n",
    "f = open(datafile,'w')\n",
    "for u, its in itemsPerUser.items():\n",
    "    for i in its:\n",
    "        print(default_formatter(u,i), file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run two stages of mapreduce\n",
    "mapper = Mapper(user_item_counts)\n",
    "mapreduce(datafile, mapout1, mapper=mapper, reducer=reducer)\n",
    "mapreduce(datafile, mapout2, mapper=indicator_mapper)  # map the data again\n",
    "mapreduce([mapout1, mapout2], outfile, reducer=indicator_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trim(u, i, j):\n",
    "    return u[1:len(u)-1], i[2:len(i)-2], j[1:len(j)-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data(filepath):\n",
    "    f = open(filepath)\n",
    "    samples = [map(str, line.strip().split()) for line in f]\n",
    "    return [trim(u, i, j) for u, i, j in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_random_batches(data, batch_size=1024):\n",
    "    batches = []\n",
    "    random.shuffle(data)\n",
    "    num_batches = int(len(data)/batch_size)\n",
    "    for i in range(num_batches):\n",
    "        mini_batch = data[i*batch_size:(i+1)*batch_size]\n",
    "        batches.append(mini_batch)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = create_data(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inner(x, y):\n",
    "    return sum([a*b for a,b in zip(x,y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_label(u, i, j):\n",
    "    c = Counter(itemsPerUser[u])\n",
    "    if c[i] >= c[j]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_outputs(sample):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for u, i, j in sample:\n",
    "        predict = sigmoid(prediction(u, i, j))\n",
    "        label = binary_label(u, i, j)\n",
    "        predictions.append(predict)\n",
    "        labels.append(label)\n",
    "            \n",
    "    return np.rint(predictions), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    differences = [1 if x == y else 0 for x, y in zip(predictions, labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = [binary_label(u, i, j) for u, i, j in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_labels = [binary_label(u, i, j) for u, i, j in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mini_batches = create_random_batches(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    #Numerically stable sigmoid function.\n",
    "    #Taken from: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    if x >= 0:\n",
    "        z = np.exp(-x)\n",
    "        return 1 / (1 + z)\n",
    "    else:\n",
    "        # if x is less than zero then z will be small, denom can't be\n",
    "        # zero because it's 1+z.\n",
    "        z = np.exp(x)\n",
    "        return z / (1 + z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple (Biase Only) Latent Factor Model with Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itemBiases = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global itemBiases\n",
    "    itemBiases = dict(zip(items, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "f(i, j) = \\beta_i - \\beta_j\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "p(i >_u j) = \\sigma(f(i, j))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(u, item_i, item_j):\n",
    "    return itemBiases[item_i] - itemBiases[item_j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\text{Cost Function (arg min)}:= \\sum_{u,i,j} -\\ln(\\sigma(\\beta_i - \\beta_j)) + \\lambda \\sum_i \\beta_i^2 = \\sum_{u,i,j} -ln\\left( \\frac{1}{1 + e^{\\beta_j - \\beta_i}} \\right) + \\lambda \\sum_i \\beta_i^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Cost(object):\n",
    "    def __init__(self, max_iter=20000):\n",
    "        self.max_iter = max_iter\n",
    "        self.nit = 0\n",
    "        \n",
    "    def __call__(self, theta, lamb):\n",
    "        if self.nit >= self.max_iter:\n",
    "            raise RuntimeError(\"max iteration reached\")\n",
    "        self.nit += 1\n",
    "        unpack(theta)\n",
    "        cost = 0.0\n",
    "        predictions = []\n",
    "        for u, i, j in train_data:\n",
    "            x = prediction(u, i, j)\n",
    "            predictions.append(sigmoid(x))\n",
    "            cost += np.log(sigmoid(x))\n",
    "\n",
    "        for i in itemBiases:\n",
    "            cost -= lamb*itemBiases[i]**2\n",
    "\n",
    "        print('iteration {0} Cost: {1}'.format(self.nit, -cost))\n",
    "        print('iteration {0} Training Accuracy: {1}'.format(self.nit, accuracy(np.rint(predictions), train_labels[:500000])))\n",
    "        print('-------------------------------------------------------------------')\n",
    "\n",
    "        return -cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = Cost()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial }{\\partial x} ln\\sigma(x) = \\frac{1}{1 + e^x} = \\sigma(-x)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\beta_i} = -\\frac{e^{\\beta_j - \\beta_i}}{1 + e^{\\beta_j - \\beta_i}} + 2 \\lambda \\beta_j = -\\frac{1}{1 + e^{\\beta_i - \\beta_j}} + 2 \\lambda \\beta_j\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\beta_j} = \\frac{e^{\\beta_j - \\beta_i}}{1 + e^{\\beta_j - \\beta_i}} + 2 \\lambda \\beta_j = \\frac{1}{1 + e^{\\beta_i - \\beta_j}} + 2 \\lambda \\beta_j\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Important]: switch the sign of all partial derivatives to compute your gradient ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def derivative(theta, lamb):\n",
    "    unpack(theta)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    for u, i, j in train_data:\n",
    "        x = prediction(u, i, j)\n",
    "        dbase = 1 / (1 + np.exp(x)) # negative gradient descent for maximizing\n",
    "        dItemBiases[i] += -dbase\n",
    "        dItemBiases[j] += dbase\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "    dtheta = [dItemBiases[i] for i in items]\n",
    "    return np.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 Cost: 881087.1071146384\n",
      "iteration 1 Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "iteration 2 Cost: 870261.4208830316\n",
      "iteration 2 Training Accuracy: 0.567466\n",
      "-------------------------------------------------------------------\n",
      "iteration 3 Cost: 850370.8177994409\n",
      "iteration 3 Training Accuracy: 0.575028\n",
      "-------------------------------------------------------------------\n",
      "iteration 4 Cost: 848075.8628756106\n",
      "iteration 4 Training Accuracy: 0.579118\n",
      "-------------------------------------------------------------------\n",
      "iteration 5 Cost: 846492.8089477586\n",
      "iteration 5 Training Accuracy: 0.58264\n",
      "-------------------------------------------------------------------\n",
      "iteration 6 Cost: 845749.7782593477\n",
      "iteration 6 Training Accuracy: 0.584818\n",
      "-------------------------------------------------------------------\n",
      "iteration 7 Cost: 845136.7970781651\n",
      "iteration 7 Training Accuracy: 0.585248\n",
      "-------------------------------------------------------------------\n",
      "iteration 8 Cost: 844844.0712379167\n",
      "iteration 8 Training Accuracy: 0.585624\n",
      "-------------------------------------------------------------------\n",
      "iteration 9 Cost: 844478.3483826199\n",
      "iteration 9 Training Accuracy: 0.586252\n",
      "-------------------------------------------------------------------\n",
      "iteration 10 Cost: 844106.4046311034\n",
      "iteration 10 Training Accuracy: 0.58669\n",
      "-------------------------------------------------------------------\n",
      "iteration 11 Cost: 843785.2759131525\n",
      "iteration 11 Training Accuracy: 0.587138\n",
      "-------------------------------------------------------------------\n",
      "iteration 12 Cost: 843606.0423629787\n",
      "iteration 12 Training Accuracy: 0.58732\n",
      "-------------------------------------------------------------------\n",
      "iteration 13 Cost: 843362.9809504229\n",
      "iteration 13 Training Accuracy: 0.58758\n",
      "-------------------------------------------------------------------\n",
      "iteration 14 Cost: 843249.5561132476\n",
      "iteration 14 Training Accuracy: 0.5874\n",
      "-------------------------------------------------------------------\n",
      "iteration 15 Cost: 843086.1940435016\n",
      "iteration 15 Training Accuracy: 0.587718\n",
      "-------------------------------------------------------------------\n",
      "iteration 16 Cost: 842951.7185145849\n",
      "iteration 16 Training Accuracy: 0.587818\n",
      "-------------------------------------------------------------------\n",
      "iteration 17 Cost: 842855.3623105312\n",
      "iteration 17 Training Accuracy: 0.587764\n",
      "-------------------------------------------------------------------\n",
      "iteration 18 Cost: 842679.0638069032\n",
      "iteration 18 Training Accuracy: 0.588116\n",
      "-------------------------------------------------------------------\n",
      "iteration 19 Cost: 842592.7288344475\n",
      "iteration 19 Training Accuracy: 0.587722\n",
      "-------------------------------------------------------------------\n",
      "iteration 20 Cost: 842479.7750055753\n",
      "iteration 20 Training Accuracy: 0.588032\n",
      "-------------------------------------------------------------------\n",
      "iteration 21 Cost: 842423.1868699046\n",
      "iteration 21 Training Accuracy: 0.588048\n",
      "-------------------------------------------------------------------\n",
      "iteration 22 Cost: 842341.6542678059\n",
      "iteration 22 Training Accuracy: 0.587878\n",
      "-------------------------------------------------------------------\n",
      "iteration 23 Cost: 842233.6337309664\n",
      "iteration 23 Training Accuracy: 0.588138\n",
      "-------------------------------------------------------------------\n",
      "iteration 24 Cost: 842127.8186809295\n",
      "iteration 24 Training Accuracy: 0.588006\n",
      "-------------------------------------------------------------------\n",
      "iteration 25 Cost: 842068.1638872867\n",
      "iteration 25 Training Accuracy: 0.588108\n",
      "-------------------------------------------------------------------\n",
      "iteration 26 Cost: 841984.4873755176\n",
      "iteration 26 Training Accuracy: 0.588352\n",
      "-------------------------------------------------------------------\n",
      "iteration 27 Cost: 841931.2921855052\n",
      "iteration 27 Training Accuracy: 0.588396\n",
      "-------------------------------------------------------------------\n",
      "iteration 28 Cost: 841871.4931453314\n",
      "iteration 28 Training Accuracy: 0.588416\n",
      "-------------------------------------------------------------------\n",
      "iteration 29 Cost: 841782.1814016559\n",
      "iteration 29 Training Accuracy: 0.588602\n",
      "-------------------------------------------------------------------\n",
      "iteration 30 Cost: 841741.6642753195\n",
      "iteration 30 Training Accuracy: 0.58862\n",
      "-------------------------------------------------------------------\n",
      "iteration 31 Cost: 841690.1828265716\n",
      "iteration 31 Training Accuracy: 0.588562\n",
      "-------------------------------------------------------------------\n",
      "iteration 32 Cost: 841632.4252861238\n",
      "iteration 32 Training Accuracy: 0.588662\n",
      "-------------------------------------------------------------------\n",
      "iteration 33 Cost: 841579.260322335\n",
      "iteration 33 Training Accuracy: 0.588624\n",
      "-------------------------------------------------------------------\n",
      "iteration 34 Cost: 841555.5760659804\n",
      "iteration 34 Training Accuracy: 0.588698\n",
      "-------------------------------------------------------------------\n",
      "iteration 35 Cost: 841489.8177519679\n",
      "iteration 35 Training Accuracy: 0.588444\n",
      "-------------------------------------------------------------------\n",
      "iteration 36 Cost: 841482.0942842996\n",
      "iteration 36 Training Accuracy: 0.588404\n",
      "-------------------------------------------------------------------\n",
      "iteration 37 Cost: 841422.2246641066\n",
      "iteration 37 Training Accuracy: 0.588538\n",
      "-------------------------------------------------------------------\n",
      "iteration 38 Cost: 841399.4463775025\n",
      "iteration 38 Training Accuracy: 0.58865\n",
      "-------------------------------------------------------------------\n",
      "iteration 39 Cost: 841366.1597743513\n",
      "iteration 39 Training Accuracy: 0.588814\n",
      "-------------------------------------------------------------------\n",
      "iteration 40 Cost: 841320.1998529873\n",
      "iteration 40 Training Accuracy: 0.588882\n",
      "-------------------------------------------------------------------\n",
      "iteration 41 Cost: 841360.979896766\n",
      "iteration 41 Training Accuracy: 0.5889\n",
      "-------------------------------------------------------------------\n",
      "iteration 42 Cost: 841294.4885977615\n",
      "iteration 42 Training Accuracy: 0.58897\n",
      "-------------------------------------------------------------------\n",
      "iteration 43 Cost: 841252.2636607003\n",
      "iteration 43 Training Accuracy: 0.588884\n",
      "-------------------------------------------------------------------\n",
      "iteration 44 Cost: 841224.3412860674\n",
      "iteration 44 Training Accuracy: 0.58888\n",
      "-------------------------------------------------------------------\n",
      "iteration 45 Cost: 841188.2757415279\n",
      "iteration 45 Training Accuracy: 0.589034\n",
      "-------------------------------------------------------------------\n",
      "iteration 46 Cost: 841169.2238388377\n",
      "iteration 46 Training Accuracy: 0.589018\n",
      "-------------------------------------------------------------------\n",
      "iteration 47 Cost: 841136.7621099264\n",
      "iteration 47 Training Accuracy: 0.589152\n",
      "-------------------------------------------------------------------\n",
      "iteration 48 Cost: 841112.4561977743\n",
      "iteration 48 Training Accuracy: 0.588982\n",
      "-------------------------------------------------------------------\n",
      "iteration 49 Cost: 841091.2344405919\n",
      "iteration 49 Training Accuracy: 0.588816\n",
      "-------------------------------------------------------------------\n",
      "iteration 50 Cost: 841075.1575216972\n",
      "iteration 50 Training Accuracy: 0.588938\n",
      "-------------------------------------------------------------------\n",
      "iteration 51 Cost: 841041.7847747174\n",
      "iteration 51 Training Accuracy: 0.5889\n",
      "-------------------------------------------------------------------\n",
      "iteration 52 Cost: 841024.968147886\n",
      "iteration 52 Training Accuracy: 0.58873\n",
      "-------------------------------------------------------------------\n",
      "iteration 53 Cost: 841005.4657530403\n",
      "iteration 53 Training Accuracy: 0.588852\n",
      "-------------------------------------------------------------------\n",
      "iteration 54 Cost: 840977.6982913901\n",
      "iteration 54 Training Accuracy: 0.588812\n",
      "-------------------------------------------------------------------\n",
      "iteration 55 Cost: 841013.2553423179\n",
      "iteration 55 Training Accuracy: 0.588802\n",
      "-------------------------------------------------------------------\n",
      "iteration 56 Cost: 840961.8379840743\n",
      "iteration 56 Training Accuracy: 0.588776\n",
      "-------------------------------------------------------------------\n",
      "iteration 57 Cost: 840935.9988499744\n",
      "iteration 57 Training Accuracy: 0.58888\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 58 Cost: 840919.034125573\n",
      "iteration 58 Training Accuracy: 0.588972\n",
      "-------------------------------------------------------------------\n",
      "iteration 59 Cost: 840893.4818122418\n",
      "iteration 59 Training Accuracy: 0.588934\n",
      "-------------------------------------------------------------------\n",
      "iteration 60 Cost: 840877.4096184314\n",
      "iteration 60 Training Accuracy: 0.588734\n",
      "-------------------------------------------------------------------\n",
      "iteration 61 Cost: 840852.1598087591\n",
      "iteration 61 Training Accuracy: 0.588778\n",
      "-------------------------------------------------------------------\n",
      "iteration 62 Cost: 840835.7631463446\n",
      "iteration 62 Training Accuracy: 0.588744\n",
      "-------------------------------------------------------------------\n",
      "iteration 63 Cost: 840819.0969745023\n",
      "iteration 63 Training Accuracy: 0.588784\n",
      "-------------------------------------------------------------------\n",
      "iteration 64 Cost: 840803.8761601396\n",
      "iteration 64 Training Accuracy: 0.58864\n",
      "-------------------------------------------------------------------\n",
      "iteration 65 Cost: 840782.7741050253\n",
      "iteration 65 Training Accuracy: 0.58883\n",
      "-------------------------------------------------------------------\n",
      "iteration 66 Cost: 840768.2942219423\n",
      "iteration 66 Training Accuracy: 0.588856\n",
      "-------------------------------------------------------------------\n",
      "iteration 67 Cost: 840755.9939186436\n",
      "iteration 67 Training Accuracy: 0.588894\n",
      "-------------------------------------------------------------------\n",
      "iteration 68 Cost: 840743.4665889076\n",
      "iteration 68 Training Accuracy: 0.588916\n",
      "-------------------------------------------------------------------\n",
      "iteration 69 Cost: 840728.9744540042\n",
      "iteration 69 Training Accuracy: 0.58894\n",
      "-------------------------------------------------------------------\n",
      "iteration 70 Cost: 840711.0815762798\n",
      "iteration 70 Training Accuracy: 0.588972\n",
      "-------------------------------------------------------------------\n",
      "iteration 71 Cost: 840699.8624356884\n",
      "iteration 71 Training Accuracy: 0.588894\n",
      "-------------------------------------------------------------------\n",
      "iteration 72 Cost: 840687.7663491885\n",
      "iteration 72 Training Accuracy: 0.5886\n",
      "-------------------------------------------------------------------\n",
      "iteration 73 Cost: 840666.2770152063\n",
      "iteration 73 Training Accuracy: 0.588696\n",
      "-------------------------------------------------------------------\n",
      "iteration 74 Cost: 840658.6248623906\n",
      "iteration 74 Training Accuracy: 0.588808\n",
      "-------------------------------------------------------------------\n",
      "iteration 75 Cost: 840642.5109705376\n",
      "iteration 75 Training Accuracy: 0.588858\n",
      "-------------------------------------------------------------------\n",
      "iteration 76 Cost: 840641.8795052705\n",
      "iteration 76 Training Accuracy: 0.588834\n",
      "-------------------------------------------------------------------\n",
      "iteration 77 Cost: 840632.467359371\n",
      "iteration 77 Training Accuracy: 0.588818\n",
      "-------------------------------------------------------------------\n",
      "iteration 78 Cost: 840620.4398227588\n",
      "iteration 78 Training Accuracy: 0.588914\n",
      "-------------------------------------------------------------------\n",
      "iteration 79 Cost: 840605.6689728802\n",
      "iteration 79 Training Accuracy: 0.588818\n",
      "-------------------------------------------------------------------\n",
      "iteration 80 Cost: 840595.4568907527\n",
      "iteration 80 Training Accuracy: 0.588988\n",
      "-------------------------------------------------------------------\n",
      "iteration 81 Cost: 840583.5887754174\n",
      "iteration 81 Training Accuracy: 0.589012\n",
      "-------------------------------------------------------------------\n",
      "iteration 82 Cost: 840571.7857303324\n",
      "iteration 82 Training Accuracy: 0.588998\n",
      "-------------------------------------------------------------------\n",
      "iteration 83 Cost: 840563.6290443394\n",
      "iteration 83 Training Accuracy: 0.589158\n",
      "-------------------------------------------------------------------\n",
      "iteration 84 Cost: 840554.9525635247\n",
      "iteration 84 Training Accuracy: 0.589224\n",
      "-------------------------------------------------------------------\n",
      "iteration 85 Cost: 840547.7171052416\n",
      "iteration 85 Training Accuracy: 0.589194\n",
      "-------------------------------------------------------------------\n",
      "iteration 86 Cost: 840538.3943639545\n",
      "iteration 86 Training Accuracy: 0.5892\n",
      "-------------------------------------------------------------------\n",
      "iteration 87 Cost: 840526.6778235642\n",
      "iteration 87 Training Accuracy: 0.589142\n",
      "-------------------------------------------------------------------\n",
      "iteration 88 Cost: 840514.6608640067\n",
      "iteration 88 Training Accuracy: 0.589068\n",
      "-------------------------------------------------------------------\n",
      "iteration 89 Cost: 840506.4360542364\n",
      "iteration 89 Training Accuracy: 0.589018\n",
      "-------------------------------------------------------------------\n",
      "iteration 90 Cost: 840496.2949977972\n",
      "iteration 90 Training Accuracy: 0.589046\n",
      "-------------------------------------------------------------------\n",
      "iteration 91 Cost: 840488.191548651\n",
      "iteration 91 Training Accuracy: 0.58894\n",
      "-------------------------------------------------------------------\n",
      "iteration 92 Cost: 840477.1274473984\n",
      "iteration 92 Training Accuracy: 0.588986\n",
      "-------------------------------------------------------------------\n",
      "iteration 93 Cost: 840467.2401596821\n",
      "iteration 93 Training Accuracy: 0.589102\n",
      "-------------------------------------------------------------------\n",
      "iteration 94 Cost: 840457.8236301407\n",
      "iteration 94 Training Accuracy: 0.589134\n",
      "-------------------------------------------------------------------\n",
      "iteration 95 Cost: 840451.2799326504\n",
      "iteration 95 Training Accuracy: 0.589192\n",
      "-------------------------------------------------------------------\n",
      "iteration 96 Cost: 840440.6783072314\n",
      "iteration 96 Training Accuracy: 0.589226\n",
      "-------------------------------------------------------------------\n",
      "iteration 97 Cost: 840431.0793013527\n",
      "iteration 97 Training Accuracy: 0.58925\n",
      "-------------------------------------------------------------------\n",
      "iteration 98 Cost: 840425.6787180593\n",
      "iteration 98 Training Accuracy: 0.589288\n",
      "-------------------------------------------------------------------\n",
      "iteration 99 Cost: 840414.111469875\n",
      "iteration 99 Training Accuracy: 0.589284\n",
      "-------------------------------------------------------------------\n",
      "iteration 100 Cost: 840407.765310676\n",
      "iteration 100 Training Accuracy: 0.589226\n",
      "-------------------------------------------------------------------\n",
      "iteration 101 Cost: 840400.0735945896\n",
      "iteration 101 Training Accuracy: 0.589242\n",
      "-------------------------------------------------------------------\n",
      "iteration 102 Cost: 840391.1891171753\n",
      "iteration 102 Training Accuracy: 0.589246\n",
      "-------------------------------------------------------------------\n",
      "iteration 103 Cost: 840384.3173284106\n",
      "iteration 103 Training Accuracy: 0.589262\n",
      "-------------------------------------------------------------------\n",
      "iteration 104 Cost: 840376.8088389334\n",
      "iteration 104 Training Accuracy: 0.58918\n",
      "-------------------------------------------------------------------\n",
      "iteration 105 Cost: 840369.9581780875\n",
      "iteration 105 Training Accuracy: 0.589168\n",
      "-------------------------------------------------------------------\n",
      "iteration 106 Cost: 840362.2423324586\n",
      "iteration 106 Training Accuracy: 0.589102\n",
      "-------------------------------------------------------------------\n",
      "iteration 107 Cost: 840355.1770919518\n",
      "iteration 107 Training Accuracy: 0.589068\n",
      "-------------------------------------------------------------------\n",
      "iteration 108 Cost: 840351.020693705\n",
      "iteration 108 Training Accuracy: 0.58907\n",
      "-------------------------------------------------------------------\n",
      "iteration 109 Cost: 840341.8186391231\n",
      "iteration 109 Training Accuracy: 0.589014\n",
      "-------------------------------------------------------------------\n",
      "iteration 110 Cost: 840339.9076065404\n",
      "iteration 110 Training Accuracy: 0.589\n",
      "-------------------------------------------------------------------\n",
      "iteration 111 Cost: 840333.3652265558\n",
      "iteration 111 Training Accuracy: 0.589\n",
      "-------------------------------------------------------------------\n",
      "iteration 112 Cost: 840329.5191480191\n",
      "iteration 112 Training Accuracy: 0.589062\n",
      "-------------------------------------------------------------------\n",
      "iteration 113 Cost: 840324.36151175\n",
      "iteration 113 Training Accuracy: 0.58911\n",
      "-------------------------------------------------------------------\n",
      "iteration 114 Cost: 840317.5681727049\n",
      "iteration 114 Training Accuracy: 0.589064\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 115 Cost: 840313.2397350784\n",
      "iteration 115 Training Accuracy: 0.589104\n",
      "-------------------------------------------------------------------\n",
      "iteration 116 Cost: 840306.3733586288\n",
      "iteration 116 Training Accuracy: 0.589192\n",
      "-------------------------------------------------------------------\n",
      "iteration 117 Cost: 840303.7652592479\n",
      "iteration 117 Training Accuracy: 0.589218\n",
      "-------------------------------------------------------------------\n",
      "iteration 118 Cost: 840298.9875828519\n",
      "iteration 118 Training Accuracy: 0.58921\n",
      "-------------------------------------------------------------------\n",
      "iteration 119 Cost: 840293.517240242\n",
      "iteration 119 Training Accuracy: 0.588994\n",
      "-------------------------------------------------------------------\n",
      "iteration 120 Cost: 840285.3878695008\n",
      "iteration 120 Training Accuracy: 0.588974\n",
      "-------------------------------------------------------------------\n",
      "iteration 121 Cost: 840280.3950922288\n",
      "iteration 121 Training Accuracy: 0.589022\n",
      "-------------------------------------------------------------------\n",
      "iteration 122 Cost: 840275.4390085327\n",
      "iteration 122 Training Accuracy: 0.588994\n",
      "-------------------------------------------------------------------\n",
      "iteration 123 Cost: 840269.2001504853\n",
      "iteration 123 Training Accuracy: 0.588964\n",
      "-------------------------------------------------------------------\n",
      "iteration 124 Cost: 840268.0762198349\n",
      "iteration 124 Training Accuracy: 0.589012\n",
      "-------------------------------------------------------------------\n",
      "iteration 125 Cost: 840259.3638123664\n",
      "iteration 125 Training Accuracy: 0.58903\n",
      "-------------------------------------------------------------------\n",
      "iteration 126 Cost: 840256.314888426\n",
      "iteration 126 Training Accuracy: 0.58913\n",
      "-------------------------------------------------------------------\n",
      "iteration 127 Cost: 840251.6850712926\n",
      "iteration 127 Training Accuracy: 0.589152\n",
      "-------------------------------------------------------------------\n",
      "iteration 128 Cost: 840245.5534864655\n",
      "iteration 128 Training Accuracy: 0.589162\n",
      "-------------------------------------------------------------------\n",
      "iteration 129 Cost: 840239.4930825704\n",
      "iteration 129 Training Accuracy: 0.589178\n",
      "-------------------------------------------------------------------\n",
      "iteration 130 Cost: 840235.1942481297\n",
      "iteration 130 Training Accuracy: 0.589208\n",
      "-------------------------------------------------------------------\n",
      "iteration 131 Cost: 840230.470883349\n",
      "iteration 131 Training Accuracy: 0.58915\n",
      "-------------------------------------------------------------------\n",
      "iteration 132 Cost: 840225.5421892089\n",
      "iteration 132 Training Accuracy: 0.589126\n",
      "-------------------------------------------------------------------\n",
      "iteration 133 Cost: 840218.9623462739\n",
      "iteration 133 Training Accuracy: 0.589114\n",
      "-------------------------------------------------------------------\n",
      "iteration 134 Cost: 840214.6958137949\n",
      "iteration 134 Training Accuracy: 0.589148\n",
      "-------------------------------------------------------------------\n",
      "iteration 135 Cost: 840211.0248594524\n",
      "iteration 135 Training Accuracy: 0.58917\n",
      "-------------------------------------------------------------------\n",
      "iteration 136 Cost: 840205.8973801733\n",
      "iteration 136 Training Accuracy: 0.58927\n",
      "-------------------------------------------------------------------\n",
      "iteration 137 Cost: 840203.5738804854\n",
      "iteration 137 Training Accuracy: 0.589152\n",
      "-------------------------------------------------------------------\n",
      "iteration 138 Cost: 840198.0446422172\n",
      "iteration 138 Training Accuracy: 0.589152\n",
      "-------------------------------------------------------------------\n",
      "iteration 139 Cost: 840195.1685628435\n",
      "iteration 139 Training Accuracy: 0.589174\n",
      "-------------------------------------------------------------------\n",
      "iteration 140 Cost: 840191.6052289065\n",
      "iteration 140 Training Accuracy: 0.589046\n",
      "-------------------------------------------------------------------\n",
      "iteration 141 Cost: 840186.0508303135\n",
      "iteration 141 Training Accuracy: 0.588984\n",
      "-------------------------------------------------------------------\n",
      "iteration 142 Cost: 840183.0175213531\n",
      "iteration 142 Training Accuracy: 0.589026\n",
      "-------------------------------------------------------------------\n",
      "iteration 143 Cost: 840178.1724657341\n",
      "iteration 143 Training Accuracy: 0.589036\n",
      "-------------------------------------------------------------------\n",
      "iteration 144 Cost: 840175.5289476727\n",
      "iteration 144 Training Accuracy: 0.589042\n",
      "-------------------------------------------------------------------\n",
      "iteration 145 Cost: 840172.2740411947\n",
      "iteration 145 Training Accuracy: 0.589088\n",
      "-------------------------------------------------------------------\n",
      "iteration 146 Cost: 840169.6516824776\n",
      "iteration 146 Training Accuracy: 0.589088\n",
      "-------------------------------------------------------------------\n",
      "iteration 147 Cost: 840165.003344867\n",
      "iteration 147 Training Accuracy: 0.589144\n",
      "-------------------------------------------------------------------\n",
      "iteration 148 Cost: 840162.7554358691\n",
      "iteration 148 Training Accuracy: 0.589142\n",
      "-------------------------------------------------------------------\n",
      "iteration 149 Cost: 840159.9701240525\n",
      "iteration 149 Training Accuracy: 0.589168\n",
      "-------------------------------------------------------------------\n",
      "iteration 150 Cost: 840156.0011613088\n",
      "iteration 150 Training Accuracy: 0.58908\n",
      "-------------------------------------------------------------------\n",
      "iteration 151 Cost: 840151.7738608181\n",
      "iteration 151 Training Accuracy: 0.589054\n",
      "-------------------------------------------------------------------\n",
      "iteration 152 Cost: 840148.7294459094\n",
      "iteration 152 Training Accuracy: 0.589142\n",
      "-------------------------------------------------------------------\n",
      "iteration 153 Cost: 840144.4355084874\n",
      "iteration 153 Training Accuracy: 0.58917\n",
      "-------------------------------------------------------------------\n",
      "iteration 154 Cost: 840142.3267101381\n",
      "iteration 154 Training Accuracy: 0.589172\n",
      "-------------------------------------------------------------------\n",
      "iteration 155 Cost: 840139.1059692595\n",
      "iteration 155 Training Accuracy: 0.58912\n",
      "-------------------------------------------------------------------\n",
      "iteration 156 Cost: 840135.8585859156\n",
      "iteration 156 Training Accuracy: 0.589118\n",
      "-------------------------------------------------------------------\n",
      "iteration 157 Cost: 840133.6958873589\n",
      "iteration 157 Training Accuracy: 0.589158\n",
      "-------------------------------------------------------------------\n",
      "iteration 158 Cost: 840128.4669620986\n",
      "iteration 158 Training Accuracy: 0.5891\n",
      "-------------------------------------------------------------------\n",
      "iteration 159 Cost: 840126.3679336464\n",
      "iteration 159 Training Accuracy: 0.589012\n",
      "-------------------------------------------------------------------\n",
      "iteration 160 Cost: 840122.5775433251\n",
      "iteration 160 Training Accuracy: 0.589104\n",
      "-------------------------------------------------------------------\n",
      "iteration 161 Cost: 840120.9907324938\n",
      "iteration 161 Training Accuracy: 0.589144\n",
      "-------------------------------------------------------------------\n",
      "iteration 162 Cost: 840118.2342604949\n",
      "iteration 162 Training Accuracy: 0.589168\n",
      "-------------------------------------------------------------------\n",
      "iteration 163 Cost: 840115.883505898\n",
      "iteration 163 Training Accuracy: 0.589252\n",
      "-------------------------------------------------------------------\n",
      "iteration 164 Cost: 840112.5239169654\n",
      "iteration 164 Training Accuracy: 0.58929\n",
      "-------------------------------------------------------------------\n",
      "iteration 165 Cost: 840110.8938851417\n",
      "iteration 165 Training Accuracy: 0.589276\n",
      "-------------------------------------------------------------------\n",
      "iteration 166 Cost: 840108.4689672003\n",
      "iteration 166 Training Accuracy: 0.589246\n",
      "-------------------------------------------------------------------\n",
      "iteration 167 Cost: 840105.4822607746\n",
      "iteration 167 Training Accuracy: 0.58926\n",
      "-------------------------------------------------------------------\n",
      "iteration 168 Cost: 840102.4140214983\n",
      "iteration 168 Training Accuracy: 0.589066\n",
      "-------------------------------------------------------------------\n",
      "iteration 169 Cost: 840100.0746203858\n",
      "iteration 169 Training Accuracy: 0.589046\n",
      "-------------------------------------------------------------------\n",
      "iteration 170 Cost: 840097.0712003928\n",
      "iteration 170 Training Accuracy: 0.589118\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 171 Cost: 840095.0092199519\n",
      "iteration 171 Training Accuracy: 0.589164\n",
      "-------------------------------------------------------------------\n",
      "iteration 172 Cost: 840093.0040961453\n",
      "iteration 172 Training Accuracy: 0.589158\n",
      "-------------------------------------------------------------------\n",
      "iteration 173 Cost: 840089.1974416305\n",
      "iteration 173 Training Accuracy: 0.58907\n",
      "-------------------------------------------------------------------\n",
      "iteration 174 Cost: 840087.764886249\n",
      "iteration 174 Training Accuracy: 0.589084\n",
      "-------------------------------------------------------------------\n",
      "iteration 175 Cost: 840085.7097639256\n",
      "iteration 175 Training Accuracy: 0.58909\n",
      "-------------------------------------------------------------------\n",
      "iteration 176 Cost: 840083.2408641488\n",
      "iteration 176 Training Accuracy: 0.589046\n",
      "-------------------------------------------------------------------\n",
      "iteration 177 Cost: 840081.8898711638\n",
      "iteration 177 Training Accuracy: 0.589098\n",
      "-------------------------------------------------------------------\n",
      "iteration 178 Cost: 840078.8992461337\n",
      "iteration 178 Training Accuracy: 0.58908\n",
      "-------------------------------------------------------------------\n",
      "iteration 179 Cost: 840080.032193952\n",
      "iteration 179 Training Accuracy: 0.589096\n",
      "-------------------------------------------------------------------\n",
      "iteration 180 Cost: 840076.9790485355\n",
      "iteration 180 Training Accuracy: 0.589084\n",
      "-------------------------------------------------------------------\n",
      "iteration 181 Cost: 840074.8805625086\n",
      "iteration 181 Training Accuracy: 0.589126\n",
      "-------------------------------------------------------------------\n",
      "iteration 182 Cost: 840073.5163763537\n",
      "iteration 182 Training Accuracy: 0.589168\n",
      "-------------------------------------------------------------------\n",
      "iteration 183 Cost: 840071.2315680869\n",
      "iteration 183 Training Accuracy: 0.589166\n",
      "-------------------------------------------------------------------\n",
      "iteration 184 Cost: 840069.5771730522\n",
      "iteration 184 Training Accuracy: 0.58905\n",
      "-------------------------------------------------------------------\n",
      "iteration 185 Cost: 840067.460590464\n",
      "iteration 185 Training Accuracy: 0.589038\n",
      "-------------------------------------------------------------------\n",
      "iteration 186 Cost: 840065.9244560236\n",
      "iteration 186 Training Accuracy: 0.58911\n",
      "-------------------------------------------------------------------\n",
      "iteration 187 Cost: 840064.3622868286\n",
      "iteration 187 Training Accuracy: 0.589196\n",
      "-------------------------------------------------------------------\n",
      "iteration 188 Cost: 840062.2912285735\n",
      "iteration 188 Training Accuracy: 0.589136\n",
      "-------------------------------------------------------------------\n",
      "iteration 189 Cost: 840059.9307158428\n",
      "iteration 189 Training Accuracy: 0.589084\n",
      "-------------------------------------------------------------------\n",
      "iteration 190 Cost: 840058.3295702923\n",
      "iteration 190 Training Accuracy: 0.589094\n",
      "-------------------------------------------------------------------\n",
      "iteration 191 Cost: 840056.7991779385\n",
      "iteration 191 Training Accuracy: 0.589128\n",
      "-------------------------------------------------------------------\n",
      "iteration 192 Cost: 840055.5378720508\n",
      "iteration 192 Training Accuracy: 0.589116\n",
      "-------------------------------------------------------------------\n",
      "iteration 193 Cost: 840054.0781108611\n",
      "iteration 193 Training Accuracy: 0.589086\n",
      "-------------------------------------------------------------------\n",
      "iteration 194 Cost: 840052.854072458\n",
      "iteration 194 Training Accuracy: 0.589138\n",
      "-------------------------------------------------------------------\n",
      "iteration 195 Cost: 840051.4269755838\n",
      "iteration 195 Training Accuracy: 0.589226\n",
      "-------------------------------------------------------------------\n",
      "iteration 196 Cost: 840050.0025419045\n",
      "iteration 196 Training Accuracy: 0.589246\n",
      "-------------------------------------------------------------------\n",
      "iteration 197 Cost: 840048.9496722538\n",
      "iteration 197 Training Accuracy: 0.589192\n",
      "-------------------------------------------------------------------\n",
      "iteration 198 Cost: 840047.5414293\n",
      "iteration 198 Training Accuracy: 0.589118\n",
      "-------------------------------------------------------------------\n",
      "iteration 199 Cost: 840046.676578073\n",
      "iteration 199 Training Accuracy: 0.589098\n",
      "-------------------------------------------------------------------\n",
      "iteration 200 Cost: 840045.637719519\n",
      "iteration 200 Training Accuracy: 0.58915\n",
      "-------------------------------------------------------------------\n",
      "iteration 201 Cost: 840044.1558288427\n",
      "iteration 201 Training Accuracy: 0.5891\n",
      "-------------------------------------------------------------------\n",
      "iteration 202 Cost: 840043.4541689659\n",
      "iteration 202 Training Accuracy: 0.5891\n",
      "-------------------------------------------------------------------\n",
      "iteration 203 Cost: 840042.5600098161\n",
      "iteration 203 Training Accuracy: 0.5891\n",
      "-------------------------------------------------------------------\n",
      "iteration 204 Cost: 840041.0053566375\n",
      "iteration 204 Training Accuracy: 0.589076\n",
      "-------------------------------------------------------------------\n",
      "iteration 205 Cost: 840040.1502017991\n",
      "iteration 205 Training Accuracy: 0.589072\n",
      "-------------------------------------------------------------------\n",
      "iteration 206 Cost: 840039.4776133845\n",
      "iteration 206 Training Accuracy: 0.589138\n",
      "-------------------------------------------------------------------\n",
      "iteration 207 Cost: 840038.4286971844\n",
      "iteration 207 Training Accuracy: 0.589146\n",
      "-------------------------------------------------------------------\n",
      "iteration 208 Cost: 840037.6052156466\n",
      "iteration 208 Training Accuracy: 0.589114\n",
      "-------------------------------------------------------------------\n",
      "iteration 209 Cost: 840037.298200063\n",
      "iteration 209 Training Accuracy: 0.589238\n",
      "-------------------------------------------------------------------\n",
      "iteration 210 Cost: 840036.0860566944\n",
      "iteration 210 Training Accuracy: 0.589228\n",
      "-------------------------------------------------------------------\n",
      "iteration 211 Cost: 840035.5745741522\n",
      "iteration 211 Training Accuracy: 0.589258\n",
      "-------------------------------------------------------------------\n",
      "iteration 212 Cost: 840034.882605644\n",
      "iteration 212 Training Accuracy: 0.589232\n",
      "-------------------------------------------------------------------\n",
      "iteration 213 Cost: 840034.0273359664\n",
      "iteration 213 Training Accuracy: 0.58921\n",
      "-------------------------------------------------------------------\n",
      "iteration 214 Cost: 840035.2019141576\n",
      "iteration 214 Training Accuracy: 0.589192\n",
      "-------------------------------------------------------------------\n",
      "iteration 215 Cost: 840033.540809859\n",
      "iteration 215 Training Accuracy: 0.589178\n",
      "-------------------------------------------------------------------\n",
      "iteration 216 Cost: 840032.6179550226\n",
      "iteration 216 Training Accuracy: 0.589202\n",
      "-------------------------------------------------------------------\n",
      "iteration 217 Cost: 840032.0098987739\n",
      "iteration 217 Training Accuracy: 0.58916\n",
      "-------------------------------------------------------------------\n",
      "iteration 218 Cost: 840031.1682753261\n",
      "iteration 218 Training Accuracy: 0.589222\n",
      "-------------------------------------------------------------------\n",
      "iteration 219 Cost: 840030.6774136273\n",
      "iteration 219 Training Accuracy: 0.589162\n",
      "-------------------------------------------------------------------\n",
      "iteration 220 Cost: 840030.0563123509\n",
      "iteration 220 Training Accuracy: 0.589126\n",
      "-------------------------------------------------------------------\n",
      "iteration 221 Cost: 840029.4060288707\n",
      "iteration 221 Training Accuracy: 0.589204\n",
      "-------------------------------------------------------------------\n",
      "iteration 222 Cost: 840028.9404944945\n",
      "iteration 222 Training Accuracy: 0.589168\n",
      "-------------------------------------------------------------------\n",
      "iteration 223 Cost: 840028.9505137292\n",
      "iteration 223 Training Accuracy: 0.58915\n",
      "-------------------------------------------------------------------\n",
      "iteration 224 Cost: 840028.4717342905\n",
      "iteration 224 Training Accuracy: 0.58912\n",
      "-------------------------------------------------------------------\n",
      "iteration 225 Cost: 840027.7663199794\n",
      "iteration 225 Training Accuracy: 0.589144\n",
      "-------------------------------------------------------------------\n",
      "iteration 226 Cost: 840027.3227219406\n",
      "iteration 226 Training Accuracy: 0.58923\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 227 Cost: 840026.8367475986\n",
      "iteration 227 Training Accuracy: 0.589228\n",
      "-------------------------------------------------------------------\n",
      "iteration 228 Cost: 840026.3789162773\n",
      "iteration 228 Training Accuracy: 0.589224\n",
      "-------------------------------------------------------------------\n",
      "iteration 229 Cost: 840025.9154739043\n",
      "iteration 229 Training Accuracy: 0.589174\n",
      "-------------------------------------------------------------------\n",
      "iteration 230 Cost: 840025.2738612668\n",
      "iteration 230 Training Accuracy: 0.589172\n",
      "-------------------------------------------------------------------\n",
      "iteration 231 Cost: 840024.9073388462\n",
      "iteration 231 Training Accuracy: 0.589222\n",
      "-------------------------------------------------------------------\n",
      "iteration 232 Cost: 840024.5984547781\n",
      "iteration 232 Training Accuracy: 0.589196\n",
      "-------------------------------------------------------------------\n",
      "iteration 233 Cost: 840024.1329702362\n",
      "iteration 233 Training Accuracy: 0.58917\n",
      "-------------------------------------------------------------------\n",
      "iteration 234 Cost: 840023.775335336\n",
      "iteration 234 Training Accuracy: 0.58922\n",
      "-------------------------------------------------------------------\n",
      "iteration 235 Cost: 840023.410595005\n",
      "iteration 235 Training Accuracy: 0.58925\n",
      "-------------------------------------------------------------------\n",
      "iteration 236 Cost: 840022.9277260983\n",
      "iteration 236 Training Accuracy: 0.589282\n",
      "-------------------------------------------------------------------\n",
      "iteration 237 Cost: 840022.6740594566\n",
      "iteration 237 Training Accuracy: 0.589266\n",
      "-------------------------------------------------------------------\n",
      "iteration 238 Cost: 840022.3464627783\n",
      "iteration 238 Training Accuracy: 0.589178\n",
      "-------------------------------------------------------------------\n",
      "iteration 239 Cost: 840022.0944736558\n",
      "iteration 239 Training Accuracy: 0.589152\n",
      "-------------------------------------------------------------------\n",
      "iteration 240 Cost: 840021.583460592\n",
      "iteration 240 Training Accuracy: 0.589188\n",
      "-------------------------------------------------------------------\n",
      "iteration 241 Cost: 840021.2959850386\n",
      "iteration 241 Training Accuracy: 0.589166\n",
      "-------------------------------------------------------------------\n",
      "iteration 242 Cost: 840021.0093167869\n",
      "iteration 242 Training Accuracy: 0.589128\n",
      "-------------------------------------------------------------------\n",
      "iteration 243 Cost: 840020.514429103\n",
      "iteration 243 Training Accuracy: 0.589132\n",
      "-------------------------------------------------------------------\n",
      "iteration 244 Cost: 840020.7020002701\n",
      "iteration 244 Training Accuracy: 0.589106\n",
      "-------------------------------------------------------------------\n",
      "iteration 245 Cost: 840020.1954643553\n",
      "iteration 245 Training Accuracy: 0.58913\n",
      "-------------------------------------------------------------------\n",
      "iteration 246 Cost: 840019.7813976523\n",
      "iteration 246 Training Accuracy: 0.58909\n",
      "-------------------------------------------------------------------\n",
      "iteration 247 Cost: 840019.4904542768\n",
      "iteration 247 Training Accuracy: 0.58902\n",
      "-------------------------------------------------------------------\n",
      "iteration 248 Cost: 840019.0904865063\n",
      "iteration 248 Training Accuracy: 0.589132\n",
      "-------------------------------------------------------------------\n",
      "iteration 249 Cost: 840018.9414036283\n",
      "iteration 249 Training Accuracy: 0.58932\n",
      "-------------------------------------------------------------------\n",
      "iteration 250 Cost: 840018.529482861\n",
      "iteration 250 Training Accuracy: 0.589384\n",
      "-------------------------------------------------------------------\n",
      "iteration 251 Cost: 840018.3458641741\n",
      "iteration 251 Training Accuracy: 0.589358\n",
      "-------------------------------------------------------------------\n",
      "iteration 252 Cost: 840018.0876441565\n",
      "iteration 252 Training Accuracy: 0.589338\n",
      "-------------------------------------------------------------------\n",
      "iteration 253 Cost: 840017.9064933565\n",
      "iteration 253 Training Accuracy: 0.589272\n",
      "-------------------------------------------------------------------\n",
      "iteration 254 Cost: 840017.492423814\n",
      "iteration 254 Training Accuracy: 0.589262\n",
      "-------------------------------------------------------------------\n",
      "iteration 255 Cost: 840017.2816618399\n",
      "iteration 255 Training Accuracy: 0.589264\n",
      "-------------------------------------------------------------------\n",
      "iteration 256 Cost: 840017.0750771643\n",
      "iteration 256 Training Accuracy: 0.589192\n",
      "-------------------------------------------------------------------\n",
      "iteration 257 Cost: 840016.716840669\n",
      "iteration 257 Training Accuracy: 0.589188\n",
      "-------------------------------------------------------------------\n",
      "iteration 258 Cost: 840017.3239194327\n",
      "iteration 258 Training Accuracy: 0.589124\n",
      "-------------------------------------------------------------------\n",
      "iteration 259 Cost: 840016.5254433154\n",
      "iteration 259 Training Accuracy: 0.589196\n",
      "-------------------------------------------------------------------\n",
      "iteration 260 Cost: 840016.1402787565\n",
      "iteration 260 Training Accuracy: 0.58914\n",
      "-------------------------------------------------------------------\n",
      "iteration 261 Cost: 840015.9101822701\n",
      "iteration 261 Training Accuracy: 0.589138\n",
      "-------------------------------------------------------------------\n",
      "iteration 262 Cost: 840015.604423224\n",
      "iteration 262 Training Accuracy: 0.589126\n",
      "-------------------------------------------------------------------\n",
      "iteration 263 Cost: 840015.4300491852\n",
      "iteration 263 Training Accuracy: 0.589176\n",
      "-------------------------------------------------------------------\n",
      "iteration 264 Cost: 840015.1697911164\n",
      "iteration 264 Training Accuracy: 0.58922\n",
      "-------------------------------------------------------------------\n",
      "iteration 265 Cost: 840014.9282616742\n",
      "iteration 265 Training Accuracy: 0.589276\n",
      "-------------------------------------------------------------------\n",
      "iteration 266 Cost: 840014.7250293506\n",
      "iteration 266 Training Accuracy: 0.589164\n",
      "-------------------------------------------------------------------\n",
      "iteration 267 Cost: 840014.5674068365\n",
      "iteration 267 Training Accuracy: 0.589252\n",
      "-------------------------------------------------------------------\n",
      "iteration 268 Cost: 840014.2427573628\n",
      "iteration 268 Training Accuracy: 0.589202\n",
      "-------------------------------------------------------------------\n",
      "iteration 269 Cost: 840014.0710621619\n",
      "iteration 269 Training Accuracy: 0.589152\n",
      "-------------------------------------------------------------------\n",
      "iteration 270 Cost: 840013.8966299262\n",
      "iteration 270 Training Accuracy: 0.589208\n",
      "-------------------------------------------------------------------\n",
      "iteration 271 Cost: 840013.7488526211\n",
      "iteration 271 Training Accuracy: 0.589148\n",
      "-------------------------------------------------------------------\n",
      "iteration 272 Cost: 840013.5535014403\n",
      "iteration 272 Training Accuracy: 0.58916\n",
      "-------------------------------------------------------------------\n",
      "iteration 273 Cost: 840013.3668252178\n",
      "iteration 273 Training Accuracy: 0.589122\n",
      "-------------------------------------------------------------------\n",
      "iteration 274 Cost: 840013.2170400424\n",
      "iteration 274 Training Accuracy: 0.589118\n",
      "-------------------------------------------------------------------\n",
      "iteration 275 Cost: 840012.9755216428\n",
      "iteration 275 Training Accuracy: 0.58916\n",
      "-------------------------------------------------------------------\n",
      "iteration 276 Cost: 840012.7523382299\n",
      "iteration 276 Training Accuracy: 0.589186\n",
      "-------------------------------------------------------------------\n",
      "iteration 277 Cost: 840012.6259500724\n",
      "iteration 277 Training Accuracy: 0.589222\n",
      "-------------------------------------------------------------------\n",
      "iteration 278 Cost: 840012.4303105657\n",
      "iteration 278 Training Accuracy: 0.589218\n",
      "-------------------------------------------------------------------\n",
      "iteration 279 Cost: 840012.2994203713\n",
      "iteration 279 Training Accuracy: 0.589208\n",
      "-------------------------------------------------------------------\n",
      "iteration 280 Cost: 840012.1335866551\n",
      "iteration 280 Training Accuracy: 0.589176\n",
      "-------------------------------------------------------------------\n",
      "iteration 281 Cost: 840011.9480700742\n",
      "iteration 281 Training Accuracy: 0.589204\n",
      "-------------------------------------------------------------------\n",
      "iteration 282 Cost: 840011.8476535046\n",
      "iteration 282 Training Accuracy: 0.589236\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 283 Cost: 840011.6712656767\n",
      "iteration 283 Training Accuracy: 0.58919\n",
      "-------------------------------------------------------------------\n",
      "iteration 284 Cost: 840011.5216297696\n",
      "iteration 284 Training Accuracy: 0.589172\n",
      "-------------------------------------------------------------------\n",
      "iteration 285 Cost: 840011.3882955728\n",
      "iteration 285 Training Accuracy: 0.589174\n",
      "-------------------------------------------------------------------\n",
      "iteration 286 Cost: 840011.2661723107\n",
      "iteration 286 Training Accuracy: 0.589168\n",
      "-------------------------------------------------------------------\n",
      "iteration 287 Cost: 840011.1589454269\n",
      "iteration 287 Training Accuracy: 0.589186\n",
      "-------------------------------------------------------------------\n",
      "iteration 288 Cost: 840011.0150325238\n",
      "iteration 288 Training Accuracy: 0.589176\n",
      "-------------------------------------------------------------------\n",
      "iteration 289 Cost: 840010.883751741\n",
      "iteration 289 Training Accuracy: 0.58919\n",
      "-------------------------------------------------------------------\n",
      "iteration 290 Cost: 840010.7828989584\n",
      "iteration 290 Training Accuracy: 0.589148\n",
      "-------------------------------------------------------------------\n",
      "iteration 291 Cost: 840010.6665598854\n",
      "iteration 291 Training Accuracy: 0.58919\n",
      "-------------------------------------------------------------------\n",
      "iteration 292 Cost: 840010.5660582865\n",
      "iteration 292 Training Accuracy: 0.589214\n",
      "-------------------------------------------------------------------\n",
      "iteration 293 Cost: 840010.4639346013\n",
      "iteration 293 Training Accuracy: 0.589188\n",
      "-------------------------------------------------------------------\n",
      "iteration 294 Cost: 840010.360376799\n",
      "iteration 294 Training Accuracy: 0.589196\n",
      "-------------------------------------------------------------------\n",
      "iteration 295 Cost: 840010.2916978829\n",
      "iteration 295 Training Accuracy: 0.589188\n",
      "-------------------------------------------------------------------\n",
      "iteration 296 Cost: 840010.163662753\n",
      "iteration 296 Training Accuracy: 0.589174\n",
      "-------------------------------------------------------------------\n",
      "iteration 297 Cost: 840010.0804266069\n",
      "iteration 297 Training Accuracy: 0.589198\n",
      "-------------------------------------------------------------------\n",
      "iteration 298 Cost: 840010.011091618\n",
      "iteration 298 Training Accuracy: 0.589194\n",
      "-------------------------------------------------------------------\n",
      "iteration 299 Cost: 840009.9417936067\n",
      "iteration 299 Training Accuracy: 0.589156\n",
      "-------------------------------------------------------------------\n",
      "iteration 300 Cost: 840009.8669483706\n",
      "iteration 300 Training Accuracy: 0.589212\n",
      "-------------------------------------------------------------------\n",
      "iteration 301 Cost: 840009.772309789\n",
      "iteration 301 Training Accuracy: 0.589218\n",
      "-------------------------------------------------------------------\n",
      "iteration 302 Cost: 840009.6877335499\n",
      "iteration 302 Training Accuracy: 0.589228\n",
      "-------------------------------------------------------------------\n",
      "iteration 303 Cost: 840009.5966399747\n",
      "iteration 303 Training Accuracy: 0.5892\n",
      "-------------------------------------------------------------------\n",
      "iteration 304 Cost: 840009.5280883637\n",
      "iteration 304 Training Accuracy: 0.589194\n",
      "-------------------------------------------------------------------\n",
      "iteration 305 Cost: 840009.460965261\n",
      "iteration 305 Training Accuracy: 0.58916\n",
      "-------------------------------------------------------------------\n",
      "iteration 306 Cost: 840009.3766248102\n",
      "iteration 306 Training Accuracy: 0.58917\n",
      "-------------------------------------------------------------------\n",
      "iteration 307 Cost: 840009.3106961264\n",
      "iteration 307 Training Accuracy: 0.589128\n",
      "-------------------------------------------------------------------\n",
      "iteration 308 Cost: 840009.2364267962\n",
      "iteration 308 Training Accuracy: 0.589098\n",
      "-------------------------------------------------------------------\n",
      "iteration 309 Cost: 840009.1626207879\n",
      "iteration 309 Training Accuracy: 0.589146\n",
      "-------------------------------------------------------------------\n",
      "iteration 310 Cost: 840009.0683939132\n",
      "iteration 310 Training Accuracy: 0.589158\n",
      "-------------------------------------------------------------------\n",
      "iteration 311 Cost: 840009.0236928476\n",
      "iteration 311 Training Accuracy: 0.58919\n",
      "-------------------------------------------------------------------\n",
      "iteration 312 Cost: 840008.913293604\n",
      "iteration 312 Training Accuracy: 0.589124\n",
      "-------------------------------------------------------------------\n",
      "iteration 313 Cost: 840008.912935399\n",
      "iteration 313 Training Accuracy: 0.589146\n",
      "-------------------------------------------------------------------\n",
      "iteration 314 Cost: 840008.8657983439\n",
      "iteration 314 Training Accuracy: 0.589116\n",
      "-------------------------------------------------------------------\n",
      "iteration 315 Cost: 840008.8005288797\n",
      "iteration 315 Training Accuracy: 0.589144\n",
      "-------------------------------------------------------------------\n",
      "iteration 316 Cost: 840008.7086083937\n",
      "iteration 316 Training Accuracy: 0.589174\n",
      "-------------------------------------------------------------------\n",
      "iteration 317 Cost: 840008.6534914584\n",
      "iteration 317 Training Accuracy: 0.589168\n",
      "-------------------------------------------------------------------\n",
      "iteration 318 Cost: 840008.6010389718\n",
      "iteration 318 Training Accuracy: 0.589172\n",
      "-------------------------------------------------------------------\n",
      "iteration 319 Cost: 840008.524388848\n",
      "iteration 319 Training Accuracy: 0.589156\n",
      "-------------------------------------------------------------------\n",
      "iteration 320 Cost: 840008.4679730663\n",
      "iteration 320 Training Accuracy: 0.589162\n",
      "-------------------------------------------------------------------\n",
      "iteration 321 Cost: 840008.4010513657\n",
      "iteration 321 Training Accuracy: 0.589214\n",
      "-------------------------------------------------------------------\n",
      "iteration 322 Cost: 840008.3028670154\n",
      "iteration 322 Training Accuracy: 0.589172\n",
      "-------------------------------------------------------------------\n",
      "iteration 323 Cost: 840008.2592637824\n",
      "iteration 323 Training Accuracy: 0.589152\n",
      "-------------------------------------------------------------------\n",
      "iteration 324 Cost: 840008.1990966222\n",
      "iteration 324 Training Accuracy: 0.589136\n",
      "-------------------------------------------------------------------\n",
      "iteration 325 Cost: 840008.1424785837\n",
      "iteration 325 Training Accuracy: 0.589142\n",
      "-------------------------------------------------------------------\n",
      "iteration 326 Cost: 840008.069148455\n",
      "iteration 326 Training Accuracy: 0.58913\n",
      "-------------------------------------------------------------------\n",
      "iteration 327 Cost: 840008.0007648484\n",
      "iteration 327 Training Accuracy: 0.589138\n",
      "-------------------------------------------------------------------\n",
      "iteration 328 Cost: 840007.9475129164\n",
      "iteration 328 Training Accuracy: 0.589162\n",
      "-------------------------------------------------------------------\n",
      "iteration 329 Cost: 840007.8882155177\n",
      "iteration 329 Training Accuracy: 0.589182\n",
      "-------------------------------------------------------------------\n",
      "iteration 330 Cost: 840007.8280159187\n",
      "iteration 330 Training Accuracy: 0.589192\n",
      "-------------------------------------------------------------------\n",
      "iteration 331 Cost: 840007.755948886\n",
      "iteration 331 Training Accuracy: 0.589214\n",
      "-------------------------------------------------------------------\n",
      "iteration 332 Cost: 840007.7045252753\n",
      "iteration 332 Training Accuracy: 0.58919\n",
      "-------------------------------------------------------------------\n",
      "iteration 333 Cost: 840007.6235555828\n",
      "iteration 333 Training Accuracy: 0.589164\n",
      "-------------------------------------------------------------------\n",
      "iteration 334 Cost: 840007.5637275581\n",
      "iteration 334 Training Accuracy: 0.589178\n",
      "-------------------------------------------------------------------\n",
      "iteration 335 Cost: 840007.4660583653\n",
      "iteration 335 Training Accuracy: 0.589214\n",
      "-------------------------------------------------------------------\n",
      "iteration 336 Cost: 840007.4120104187\n",
      "iteration 336 Training Accuracy: 0.5892\n",
      "-------------------------------------------------------------------\n",
      "iteration 337 Cost: 840007.3466936689\n",
      "iteration 337 Training Accuracy: 0.589184\n",
      "-------------------------------------------------------------------\n",
      "iteration 338 Cost: 840007.3083255548\n",
      "iteration 338 Training Accuracy: 0.589222\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 339 Cost: 840007.2162709747\n",
      "iteration 339 Training Accuracy: 0.589208\n",
      "-------------------------------------------------------------------\n",
      "iteration 340 Cost: 840007.1562234942\n",
      "iteration 340 Training Accuracy: 0.589202\n",
      "-------------------------------------------------------------------\n",
      "iteration 341 Cost: 840007.0949396899\n",
      "iteration 341 Training Accuracy: 0.589182\n",
      "-------------------------------------------------------------------\n",
      "iteration 342 Cost: 840006.9950149393\n",
      "iteration 342 Training Accuracy: 0.589148\n",
      "-------------------------------------------------------------------\n",
      "iteration 343 Cost: 840007.16666642\n",
      "iteration 343 Training Accuracy: 0.589138\n",
      "-------------------------------------------------------------------\n",
      "iteration 344 Cost: 840006.9457258992\n",
      "iteration 344 Training Accuracy: 0.58915\n",
      "-------------------------------------------------------------------\n",
      "iteration 345 Cost: 840006.8437636556\n",
      "iteration 345 Training Accuracy: 0.58914\n",
      "-------------------------------------------------------------------\n",
      "iteration 346 Cost: 840006.7746284847\n",
      "iteration 346 Training Accuracy: 0.589156\n",
      "-------------------------------------------------------------------\n",
      "iteration 347 Cost: 840006.693144622\n",
      "iteration 347 Training Accuracy: 0.589192\n",
      "-------------------------------------------------------------------\n",
      "iteration 348 Cost: 840006.6216588964\n",
      "iteration 348 Training Accuracy: 0.589186\n",
      "-------------------------------------------------------------------\n",
      "iteration 349 Cost: 840006.5584758768\n",
      "iteration 349 Training Accuracy: 0.589206\n",
      "-------------------------------------------------------------------\n",
      "iteration 350 Cost: 840006.4708745932\n",
      "iteration 350 Training Accuracy: 0.58919\n",
      "-------------------------------------------------------------------\n",
      "iteration 351 Cost: 840006.4185090337\n",
      "iteration 351 Training Accuracy: 0.58916\n",
      "-------------------------------------------------------------------\n",
      "iteration 352 Cost: 840006.322829204\n",
      "iteration 352 Training Accuracy: 0.58914\n",
      "-------------------------------------------------------------------\n",
      "iteration 353 Cost: 840006.2601025022\n",
      "iteration 353 Training Accuracy: 0.589138\n",
      "-------------------------------------------------------------------\n",
      "iteration 354 Cost: 840006.1904305513\n",
      "iteration 354 Training Accuracy: 0.58914\n",
      "-------------------------------------------------------------------\n",
      "iteration 355 Cost: 840006.1208213057\n",
      "iteration 355 Training Accuracy: 0.589164\n",
      "-------------------------------------------------------------------\n",
      "iteration 356 Cost: 840006.0134474222\n",
      "iteration 356 Training Accuracy: 0.589188\n",
      "-------------------------------------------------------------------\n",
      "iteration 357 Cost: 840005.9545137071\n",
      "iteration 357 Training Accuracy: 0.589186\n",
      "-------------------------------------------------------------------\n",
      "iteration 358 Cost: 840005.895000497\n",
      "iteration 358 Training Accuracy: 0.589202\n",
      "-------------------------------------------------------------------\n",
      "iteration 359 Cost: 840005.8137271572\n",
      "iteration 359 Training Accuracy: 0.589166\n",
      "-------------------------------------------------------------------\n",
      "iteration 360 Cost: 840005.7347788822\n",
      "iteration 360 Training Accuracy: 0.589158\n",
      "-------------------------------------------------------------------\n",
      "iteration 361 Cost: 840005.661773465\n",
      "iteration 361 Training Accuracy: 0.589144\n",
      "-------------------------------------------------------------------\n",
      "iteration 362 Cost: 840005.602517966\n",
      "iteration 362 Training Accuracy: 0.589118\n",
      "-------------------------------------------------------------------\n",
      "iteration 363 Cost: 840005.5463199613\n",
      "iteration 363 Training Accuracy: 0.589102\n",
      "-------------------------------------------------------------------\n",
      "iteration 364 Cost: 840005.4863926147\n",
      "iteration 364 Training Accuracy: 0.589118\n",
      "-------------------------------------------------------------------\n",
      "iteration 365 Cost: 840005.3543597166\n",
      "iteration 365 Training Accuracy: 0.589156\n",
      "-------------------------------------------------------------------\n",
      "iteration 366 Cost: 840005.3112746723\n",
      "iteration 366 Training Accuracy: 0.589162\n",
      "-------------------------------------------------------------------\n",
      "iteration 367 Cost: 840005.2445463316\n",
      "iteration 367 Training Accuracy: 0.58917\n",
      "-------------------------------------------------------------------\n",
      "iteration 368 Cost: 840005.1559232414\n",
      "iteration 368 Training Accuracy: 0.589216\n",
      "-------------------------------------------------------------------\n",
      "iteration 369 Cost: 840005.0914733834\n",
      "iteration 369 Training Accuracy: 0.589242\n",
      "-------------------------------------------------------------------\n",
      "iteration 370 Cost: 840004.9953150406\n",
      "iteration 370 Training Accuracy: 0.589216\n",
      "-------------------------------------------------------------------\n",
      "iteration 371 Cost: 840004.9433571723\n",
      "iteration 371 Training Accuracy: 0.58918\n",
      "-------------------------------------------------------------------\n",
      "iteration 372 Cost: 840004.8714600562\n",
      "iteration 372 Training Accuracy: 0.589186\n",
      "-------------------------------------------------------------------\n",
      "iteration 373 Cost: 840004.8210851318\n",
      "iteration 373 Training Accuracy: 0.589164\n",
      "-------------------------------------------------------------------\n",
      "iteration 374 Cost: 840004.723776007\n",
      "iteration 374 Training Accuracy: 0.589192\n",
      "-------------------------------------------------------------------\n",
      "iteration 375 Cost: 840004.6702438698\n",
      "iteration 375 Training Accuracy: 0.589214\n",
      "-------------------------------------------------------------------\n",
      "iteration 376 Cost: 840004.6138483968\n",
      "iteration 376 Training Accuracy: 0.589206\n",
      "-------------------------------------------------------------------\n",
      "iteration 377 Cost: 840004.5892320623\n",
      "iteration 377 Training Accuracy: 0.589186\n",
      "-------------------------------------------------------------------\n",
      "iteration 378 Cost: 840004.4892318534\n",
      "iteration 378 Training Accuracy: 0.589196\n",
      "-------------------------------------------------------------------\n",
      "iteration 379 Cost: 840004.4399144918\n",
      "iteration 379 Training Accuracy: 0.589218\n",
      "-------------------------------------------------------------------\n",
      "iteration 380 Cost: 840004.387714974\n",
      "iteration 380 Training Accuracy: 0.589192\n",
      "-------------------------------------------------------------------\n",
      "iteration 381 Cost: 840004.2974759925\n",
      "iteration 381 Training Accuracy: 0.589198\n",
      "-------------------------------------------------------------------\n",
      "iteration 382 Cost: 840004.2212770244\n",
      "iteration 382 Training Accuracy: 0.589218\n",
      "-------------------------------------------------------------------\n",
      "iteration 383 Cost: 840004.0530796986\n",
      "iteration 383 Training Accuracy: 0.589232\n",
      "-------------------------------------------------------------------\n",
      "iteration 384 Cost: 840003.989617859\n",
      "iteration 384 Training Accuracy: 0.589258\n",
      "-------------------------------------------------------------------\n",
      "iteration 385 Cost: 840003.8862985452\n",
      "iteration 385 Training Accuracy: 0.589264\n",
      "-------------------------------------------------------------------\n",
      "iteration 386 Cost: 840003.7771662351\n",
      "iteration 386 Training Accuracy: 0.58923\n",
      "-------------------------------------------------------------------\n",
      "iteration 387 Cost: 840003.6552105428\n",
      "iteration 387 Training Accuracy: 0.589174\n",
      "-------------------------------------------------------------------\n",
      "iteration 388 Cost: 840003.5703912625\n",
      "iteration 388 Training Accuracy: 0.589156\n",
      "-------------------------------------------------------------------\n",
      "iteration 389 Cost: 840003.4824335068\n",
      "iteration 389 Training Accuracy: 0.589126\n",
      "-------------------------------------------------------------------\n",
      "iteration 390 Cost: 840003.4028015886\n",
      "iteration 390 Training Accuracy: 0.589136\n",
      "-------------------------------------------------------------------\n",
      "iteration 391 Cost: 840003.280624138\n",
      "iteration 391 Training Accuracy: 0.589146\n",
      "-------------------------------------------------------------------\n",
      "iteration 392 Cost: 840003.2227771976\n",
      "iteration 392 Training Accuracy: 0.589204\n",
      "-------------------------------------------------------------------\n",
      "iteration 393 Cost: 840003.1678940261\n",
      "iteration 393 Training Accuracy: 0.589222\n",
      "-------------------------------------------------------------------\n",
      "iteration 394 Cost: 840003.1095529561\n",
      "iteration 394 Training Accuracy: 0.5892\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 395 Cost: 840003.0113779628\n",
      "iteration 395 Training Accuracy: 0.589176\n",
      "-------------------------------------------------------------------\n",
      "iteration 396 Cost: 840002.8986497502\n",
      "iteration 396 Training Accuracy: 0.589208\n",
      "-------------------------------------------------------------------\n",
      "iteration 397 Cost: 840002.79302395\n",
      "iteration 397 Training Accuracy: 0.589202\n",
      "-------------------------------------------------------------------\n",
      "iteration 398 Cost: 840002.7173749305\n",
      "iteration 398 Training Accuracy: 0.5892\n",
      "-------------------------------------------------------------------\n",
      "iteration 399 Cost: 840002.6135034313\n",
      "iteration 399 Training Accuracy: 0.589192\n",
      "-------------------------------------------------------------------\n",
      "iteration 400 Cost: 840002.5395778226\n",
      "iteration 400 Training Accuracy: 0.589202\n",
      "-------------------------------------------------------------------\n",
      "iteration 401 Cost: 840002.413509608\n",
      "iteration 401 Training Accuracy: 0.589252\n",
      "-------------------------------------------------------------------\n",
      "iteration 402 Cost: 840002.3535977715\n",
      "iteration 402 Training Accuracy: 0.589302\n",
      "-------------------------------------------------------------------\n",
      "iteration 403 Cost: 840002.1903947539\n",
      "iteration 403 Training Accuracy: 0.589266\n",
      "-------------------------------------------------------------------\n",
      "iteration 404 Cost: 840002.1207255746\n",
      "iteration 404 Training Accuracy: 0.589266\n",
      "-------------------------------------------------------------------\n",
      "iteration 405 Cost: 840002.0283919284\n",
      "iteration 405 Training Accuracy: 0.589254\n",
      "-------------------------------------------------------------------\n",
      "iteration 406 Cost: 840001.9337002855\n",
      "iteration 406 Training Accuracy: 0.589202\n",
      "-------------------------------------------------------------------\n",
      "iteration 407 Cost: 840001.8061038946\n",
      "iteration 407 Training Accuracy: 0.589222\n",
      "-------------------------------------------------------------------\n",
      "iteration 408 Cost: 840001.7112668196\n",
      "iteration 408 Training Accuracy: 0.589258\n",
      "-------------------------------------------------------------------\n",
      "iteration 409 Cost: 840001.629221019\n",
      "iteration 409 Training Accuracy: 0.589278\n",
      "-------------------------------------------------------------------\n",
      "iteration 410 Cost: 840001.5556630095\n",
      "iteration 410 Training Accuracy: 0.589232\n",
      "-------------------------------------------------------------------\n",
      "iteration 411 Cost: 840001.4504002093\n",
      "iteration 411 Training Accuracy: 0.5892\n",
      "-------------------------------------------------------------------\n",
      "iteration 412 Cost: 840001.3555487557\n",
      "iteration 412 Training Accuracy: 0.589202\n",
      "-------------------------------------------------------------------\n",
      "iteration 413 Cost: 840001.2810535292\n",
      "iteration 413 Training Accuracy: 0.589182\n",
      "-------------------------------------------------------------------\n",
      "iteration 414 Cost: 840001.2152234204\n",
      "iteration 414 Training Accuracy: 0.589206\n",
      "-------------------------------------------------------------------\n",
      "iteration 415 Cost: 840001.0860907602\n",
      "iteration 415 Training Accuracy: 0.58919\n",
      "-------------------------------------------------------------------\n",
      "iteration 416 Cost: 840001.0019700954\n",
      "iteration 416 Training Accuracy: 0.589152\n",
      "-------------------------------------------------------------------\n",
      "iteration 417 Cost: 840000.919277148\n",
      "iteration 417 Training Accuracy: 0.5891\n",
      "-------------------------------------------------------------------\n",
      "iteration 418 Cost: 840000.7986029148\n",
      "iteration 418 Training Accuracy: 0.589116\n",
      "-------------------------------------------------------------------\n",
      "iteration 419 Cost: 840000.7080840743\n",
      "iteration 419 Training Accuracy: 0.589174\n",
      "-------------------------------------------------------------------\n",
      "iteration 420 Cost: 840000.5957823014\n",
      "iteration 420 Training Accuracy: 0.58916\n",
      "-------------------------------------------------------------------\n",
      "iteration 421 Cost: 840000.538300588\n",
      "iteration 421 Training Accuracy: 0.589126\n",
      "-------------------------------------------------------------------\n",
      "iteration 422 Cost: 840000.4564652544\n",
      "iteration 422 Training Accuracy: 0.58912\n",
      "-------------------------------------------------------------------\n",
      "iteration 423 Cost: 840000.2982639697\n",
      "iteration 423 Training Accuracy: 0.589132\n",
      "-------------------------------------------------------------------\n",
      "iteration 424 Cost: 840000.2189563239\n",
      "iteration 424 Training Accuracy: 0.589192\n",
      "-------------------------------------------------------------------\n",
      "iteration 425 Cost: 840000.1130900257\n",
      "iteration 425 Training Accuracy: 0.589172\n",
      "-------------------------------------------------------------------\n",
      "iteration 426 Cost: 840000.0523497544\n",
      "iteration 426 Training Accuracy: 0.589146\n",
      "-------------------------------------------------------------------\n",
      "iteration 427 Cost: 839999.9745528048\n",
      "iteration 427 Training Accuracy: 0.589142\n",
      "-------------------------------------------------------------------\n",
      "iteration 428 Cost: 839999.943290057\n",
      "iteration 428 Training Accuracy: 0.589216\n",
      "-------------------------------------------------------------------\n",
      "iteration 429 Cost: 839999.8045176537\n",
      "iteration 429 Training Accuracy: 0.589204\n",
      "-------------------------------------------------------------------\n",
      "iteration 430 Cost: 839999.7490238529\n",
      "iteration 430 Training Accuracy: 0.589192\n",
      "-------------------------------------------------------------------\n",
      "iteration 431 Cost: 839999.693479059\n",
      "iteration 431 Training Accuracy: 0.589186\n",
      "-------------------------------------------------------------------\n",
      "iteration 432 Cost: 839999.5990935109\n",
      "iteration 432 Training Accuracy: 0.589174\n",
      "-------------------------------------------------------------------\n",
      "iteration 433 Cost: 839999.4940047868\n",
      "iteration 433 Training Accuracy: 0.589158\n",
      "-------------------------------------------------------------------\n",
      "iteration 434 Cost: 839999.3460217173\n",
      "iteration 434 Training Accuracy: 0.589168\n",
      "-------------------------------------------------------------------\n",
      "iteration 435 Cost: 839999.2875343862\n",
      "iteration 435 Training Accuracy: 0.58918\n",
      "-------------------------------------------------------------------\n",
      "iteration 436 Cost: 839999.2101555933\n",
      "iteration 436 Training Accuracy: 0.589158\n",
      "-------------------------------------------------------------------\n",
      "iteration 437 Cost: 839999.1535465223\n",
      "iteration 437 Training Accuracy: 0.589148\n",
      "-------------------------------------------------------------------\n",
      "iteration 438 Cost: 839999.0836913771\n",
      "iteration 438 Training Accuracy: 0.589126\n",
      "-------------------------------------------------------------------\n",
      "iteration 439 Cost: 839998.9708698395\n",
      "iteration 439 Training Accuracy: 0.589134\n",
      "-------------------------------------------------------------------\n",
      "iteration 440 Cost: 839998.9101249209\n",
      "iteration 440 Training Accuracy: 0.58915\n",
      "-------------------------------------------------------------------\n",
      "iteration 441 Cost: 839998.7962984609\n",
      "iteration 441 Training Accuracy: 0.58917\n",
      "-------------------------------------------------------------------\n",
      "iteration 442 Cost: 839998.7212721637\n",
      "iteration 442 Training Accuracy: 0.589178\n",
      "-------------------------------------------------------------------\n",
      "iteration 443 Cost: 839998.6477024058\n",
      "iteration 443 Training Accuracy: 0.589196\n",
      "-------------------------------------------------------------------\n",
      "iteration 444 Cost: 839998.5687715934\n",
      "iteration 444 Training Accuracy: 0.589244\n",
      "-------------------------------------------------------------------\n",
      "iteration 445 Cost: 839998.4893915228\n",
      "iteration 445 Training Accuracy: 0.589244\n",
      "-------------------------------------------------------------------\n",
      "iteration 446 Cost: 839998.4276406848\n",
      "iteration 446 Training Accuracy: 0.589214\n",
      "-------------------------------------------------------------------\n",
      "iteration 447 Cost: 839998.3333057698\n",
      "iteration 447 Training Accuracy: 0.589196\n",
      "-------------------------------------------------------------------\n",
      "iteration 448 Cost: 839998.2851073411\n",
      "iteration 448 Training Accuracy: 0.589162\n",
      "-------------------------------------------------------------------\n",
      "iteration 449 Cost: 839998.2230175483\n",
      "iteration 449 Training Accuracy: 0.589164\n",
      "-------------------------------------------------------------------\n",
      "iteration 450 Cost: 839998.1927561696\n",
      "iteration 450 Training Accuracy: 0.589146\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 451 Cost: 839998.1175208016\n",
      "iteration 451 Training Accuracy: 0.589184\n",
      "-------------------------------------------------------------------\n",
      "iteration 452 Cost: 839998.0758058855\n",
      "iteration 452 Training Accuracy: 0.589202\n",
      "-------------------------------------------------------------------\n",
      "iteration 453 Cost: 839998.0349010329\n",
      "iteration 453 Training Accuracy: 0.589208\n",
      "-------------------------------------------------------------------\n",
      "iteration 454 Cost: 839997.9612166561\n",
      "iteration 454 Training Accuracy: 0.589254\n",
      "-------------------------------------------------------------------\n",
      "iteration 455 Cost: 839998.1135431026\n",
      "iteration 455 Training Accuracy: 0.58924\n",
      "-------------------------------------------------------------------\n",
      "iteration 456 Cost: 839997.9263754004\n",
      "iteration 456 Training Accuracy: 0.589254\n",
      "-------------------------------------------------------------------\n",
      "iteration 457 Cost: 839997.8600480363\n",
      "iteration 457 Training Accuracy: 0.58921\n",
      "-------------------------------------------------------------------\n",
      "iteration 458 Cost: 839997.8110639362\n",
      "iteration 458 Training Accuracy: 0.589234\n",
      "-------------------------------------------------------------------\n",
      "iteration 459 Cost: 839997.7547528391\n",
      "iteration 459 Training Accuracy: 0.589174\n",
      "-------------------------------------------------------------------\n",
      "iteration 460 Cost: 839997.7164500384\n",
      "iteration 460 Training Accuracy: 0.589148\n",
      "-------------------------------------------------------------------\n",
      "iteration 461 Cost: 839997.6794715662\n",
      "iteration 461 Training Accuracy: 0.589138\n",
      "-------------------------------------------------------------------\n",
      "iteration 462 Cost: 839997.6286467542\n",
      "iteration 462 Training Accuracy: 0.589164\n",
      "-------------------------------------------------------------------\n",
      "iteration 463 Cost: 839997.595832747\n",
      "iteration 463 Training Accuracy: 0.589152\n",
      "-------------------------------------------------------------------\n",
      "iteration 464 Cost: 839997.5634978588\n",
      "iteration 464 Training Accuracy: 0.589148\n",
      "-------------------------------------------------------------------\n",
      "iteration 465 Cost: 839997.5110752238\n",
      "iteration 465 Training Accuracy: 0.589158\n",
      "-------------------------------------------------------------------\n",
      "iteration 466 Cost: 839997.4907361214\n",
      "iteration 466 Training Accuracy: 0.589166\n",
      "-------------------------------------------------------------------\n",
      "iteration 467 Cost: 839997.450460112\n",
      "iteration 467 Training Accuracy: 0.589154\n",
      "-------------------------------------------------------------------\n",
      "iteration 468 Cost: 839997.4118046822\n",
      "iteration 468 Training Accuracy: 0.58917\n",
      "-------------------------------------------------------------------\n",
      "iteration 469 Cost: 839997.3740263773\n",
      "iteration 469 Training Accuracy: 0.589198\n",
      "-------------------------------------------------------------------\n",
      "iteration 470 Cost: 839997.3456052422\n",
      "iteration 470 Training Accuracy: 0.589204\n",
      "-------------------------------------------------------------------\n",
      "iteration 471 Cost: 839997.3215939846\n",
      "iteration 471 Training Accuracy: 0.589216\n",
      "-------------------------------------------------------------------\n",
      "iteration 472 Cost: 839997.2840203759\n",
      "iteration 472 Training Accuracy: 0.589198\n",
      "-------------------------------------------------------------------\n",
      "iteration 473 Cost: 839997.2441897434\n",
      "iteration 473 Training Accuracy: 0.58924\n",
      "-------------------------------------------------------------------\n",
      "iteration 474 Cost: 839997.2228675926\n",
      "iteration 474 Training Accuracy: 0.589208\n",
      "-------------------------------------------------------------------\n",
      "iteration 475 Cost: 839997.1904964535\n",
      "iteration 475 Training Accuracy: 0.589184\n",
      "-------------------------------------------------------------------\n",
      "iteration 476 Cost: 839997.1696600101\n",
      "iteration 476 Training Accuracy: 0.589178\n",
      "-------------------------------------------------------------------\n",
      "iteration 477 Cost: 839997.145173864\n",
      "iteration 477 Training Accuracy: 0.589174\n",
      "-------------------------------------------------------------------\n",
      "iteration 478 Cost: 839997.1192863351\n",
      "iteration 478 Training Accuracy: 0.589154\n",
      "-------------------------------------------------------------------\n",
      "iteration 479 Cost: 839997.1030228247\n",
      "iteration 479 Training Accuracy: 0.58914\n",
      "-------------------------------------------------------------------\n",
      "iteration 480 Cost: 839997.0971460224\n",
      "iteration 480 Training Accuracy: 0.58914\n",
      "-------------------------------------------------------------------\n",
      "iteration 481 Cost: 839997.0558834092\n",
      "iteration 481 Training Accuracy: 0.589168\n",
      "-------------------------------------------------------------------\n",
      "iteration 482 Cost: 839997.0464638155\n",
      "iteration 482 Training Accuracy: 0.589156\n",
      "-------------------------------------------------------------------\n",
      "iteration 483 Cost: 839997.0296579336\n",
      "iteration 483 Training Accuracy: 0.589158\n",
      "-------------------------------------------------------------------\n",
      "iteration 484 Cost: 839997.0132357077\n",
      "iteration 484 Training Accuracy: 0.589162\n",
      "-------------------------------------------------------------------\n",
      "iteration 485 Cost: 839996.9955361554\n",
      "iteration 485 Training Accuracy: 0.589146\n",
      "-------------------------------------------------------------------\n",
      "iteration 486 Cost: 839996.9800628952\n",
      "iteration 486 Training Accuracy: 0.589166\n",
      "-------------------------------------------------------------------\n",
      "iteration 487 Cost: 839996.9688861558\n",
      "iteration 487 Training Accuracy: 0.589184\n",
      "-------------------------------------------------------------------\n",
      "iteration 488 Cost: 839996.9512553423\n",
      "iteration 488 Training Accuracy: 0.589156\n",
      "-------------------------------------------------------------------\n",
      "iteration 489 Cost: 839996.9305324121\n",
      "iteration 489 Training Accuracy: 0.589186\n",
      "-------------------------------------------------------------------\n",
      "iteration 490 Cost: 839996.9201281915\n",
      "iteration 490 Training Accuracy: 0.589194\n",
      "-------------------------------------------------------------------\n",
      "iteration 491 Cost: 839996.9042362579\n",
      "iteration 491 Training Accuracy: 0.589172\n",
      "-------------------------------------------------------------------\n",
      "iteration 492 Cost: 839996.8945375681\n",
      "iteration 492 Training Accuracy: 0.589134\n",
      "-------------------------------------------------------------------\n",
      "iteration 493 Cost: 839996.8757774066\n",
      "iteration 493 Training Accuracy: 0.589144\n",
      "-------------------------------------------------------------------\n",
      "iteration 494 Cost: 839996.8650863692\n",
      "iteration 494 Training Accuracy: 0.58915\n",
      "-------------------------------------------------------------------\n",
      "iteration 495 Cost: 839996.8549313599\n",
      "iteration 495 Training Accuracy: 0.58914\n",
      "-------------------------------------------------------------------\n",
      "iteration 496 Cost: 839996.8406412301\n",
      "iteration 496 Training Accuracy: 0.589148\n",
      "-------------------------------------------------------------------\n",
      "iteration 497 Cost: 839996.8253169288\n",
      "iteration 497 Training Accuracy: 0.589182\n",
      "-------------------------------------------------------------------\n",
      "iteration 498 Cost: 839996.8146584186\n",
      "iteration 498 Training Accuracy: 0.589186\n",
      "-------------------------------------------------------------------\n",
      "iteration 499 Cost: 839996.8032441471\n",
      "iteration 499 Training Accuracy: 0.589194\n",
      "-------------------------------------------------------------------\n",
      "iteration 500 Cost: 839996.792840398\n",
      "iteration 500 Training Accuracy: 0.589188\n",
      "-------------------------------------------------------------------\n",
      "iteration 501 Cost: 839996.7775724652\n",
      "iteration 501 Training Accuracy: 0.589168\n",
      "-------------------------------------------------------------------\n",
      "iteration 502 Cost: 839996.7704241436\n",
      "iteration 502 Training Accuracy: 0.589158\n",
      "-------------------------------------------------------------------\n",
      "iteration 503 Cost: 839996.755393214\n",
      "iteration 503 Training Accuracy: 0.589178\n",
      "-------------------------------------------------------------------\n",
      "iteration 504 Cost: 839996.7484841907\n",
      "iteration 504 Training Accuracy: 0.589178\n",
      "-------------------------------------------------------------------\n",
      "iteration 505 Cost: 839996.7391379905\n",
      "iteration 505 Training Accuracy: 0.589162\n",
      "-------------------------------------------------------------------\n",
      "iteration 506 Cost: 839996.7320297308\n",
      "iteration 506 Training Accuracy: 0.589166\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 507 Cost: 839996.721239212\n",
      "iteration 507 Training Accuracy: 0.589154\n",
      "-------------------------------------------------------------------\n",
      "iteration 508 Cost: 839996.7119930045\n",
      "iteration 508 Training Accuracy: 0.589164\n",
      "-------------------------------------------------------------------\n",
      "iteration 509 Cost: 839996.7038688922\n",
      "iteration 509 Training Accuracy: 0.58918\n",
      "-------------------------------------------------------------------\n",
      "iteration 510 Cost: 839996.6968813569\n",
      "iteration 510 Training Accuracy: 0.589168\n",
      "-------------------------------------------------------------------\n",
      "iteration 511 Cost: 839996.6862402176\n",
      "iteration 511 Training Accuracy: 0.589156\n",
      "-------------------------------------------------------------------\n",
      "iteration 512 Cost: 839996.6792741091\n",
      "iteration 512 Training Accuracy: 0.58915\n",
      "-------------------------------------------------------------------\n",
      "iteration 513 Cost: 839996.6727431025\n",
      "iteration 513 Training Accuracy: 0.589148\n",
      "-------------------------------------------------------------------\n",
      "iteration 514 Cost: 839996.6665102142\n",
      "iteration 514 Training Accuracy: 0.589136\n",
      "-------------------------------------------------------------------\n",
      "iteration 515 Cost: 839996.6587487422\n",
      "iteration 515 Training Accuracy: 0.589124\n",
      "-------------------------------------------------------------------\n",
      "iteration 516 Cost: 839996.6505026908\n",
      "iteration 516 Training Accuracy: 0.58912\n",
      "-------------------------------------------------------------------\n",
      "iteration 517 Cost: 839996.6448393125\n",
      "iteration 517 Training Accuracy: 0.589152\n",
      "-------------------------------------------------------------------\n",
      "iteration 518 Cost: 839996.6357332504\n",
      "iteration 518 Training Accuracy: 0.58916\n",
      "-------------------------------------------------------------------\n",
      "iteration 519 Cost: 839996.6247801757\n",
      "iteration 519 Training Accuracy: 0.589148\n",
      "-------------------------------------------------------------------\n",
      "iteration 520 Cost: 839996.61876508\n",
      "iteration 520 Training Accuracy: 0.589176\n",
      "-------------------------------------------------------------------\n",
      "iteration 521 Cost: 839996.6100240428\n",
      "iteration 521 Training Accuracy: 0.589178\n",
      "-------------------------------------------------------------------\n",
      "iteration 522 Cost: 839996.6116752868\n",
      "iteration 522 Training Accuracy: 0.589184\n",
      "-------------------------------------------------------------------\n",
      "iteration 523 Cost: 839996.6058298677\n",
      "iteration 523 Training Accuracy: 0.58917\n",
      "-------------------------------------------------------------------\n",
      "iteration 524 Cost: 839996.5997882893\n",
      "iteration 524 Training Accuracy: 0.58918\n",
      "-------------------------------------------------------------------\n",
      "iteration 525 Cost: 839996.5913715304\n",
      "iteration 525 Training Accuracy: 0.589164\n",
      "-------------------------------------------------------------------\n",
      "iteration 526 Cost: 839996.5862658769\n",
      "iteration 526 Training Accuracy: 0.589186\n",
      "-------------------------------------------------------------------\n",
      "iteration 527 Cost: 839996.5802768312\n",
      "iteration 527 Training Accuracy: 0.589166\n",
      "-------------------------------------------------------------------\n",
      "iteration 528 Cost: 839996.5727409378\n",
      "iteration 528 Training Accuracy: 0.589172\n",
      "-------------------------------------------------------------------\n",
      "iteration 529 Cost: 839996.565681051\n",
      "iteration 529 Training Accuracy: 0.589168\n",
      "-------------------------------------------------------------------\n",
      "iteration 530 Cost: 839996.5596783205\n",
      "iteration 530 Training Accuracy: 0.589152\n",
      "-------------------------------------------------------------------\n",
      "iteration 531 Cost: 839996.5534557352\n",
      "iteration 531 Training Accuracy: 0.589164\n",
      "-------------------------------------------------------------------\n",
      "iteration 532 Cost: 839996.5476300697\n",
      "iteration 532 Training Accuracy: 0.589146\n",
      "-------------------------------------------------------------------\n",
      "iteration 533 Cost: 839996.5416935377\n",
      "iteration 533 Training Accuracy: 0.589144\n",
      "-------------------------------------------------------------------\n",
      "iteration 534 Cost: 839996.5389262754\n",
      "iteration 534 Training Accuracy: 0.58915\n",
      "-------------------------------------------------------------------\n",
      "iteration 535 Cost: 839996.5290465816\n",
      "iteration 535 Training Accuracy: 0.589166\n",
      "-------------------------------------------------------------------\n",
      "iteration 536 Cost: 839996.5251908797\n",
      "iteration 536 Training Accuracy: 0.589174\n",
      "-------------------------------------------------------------------\n",
      "iteration 537 Cost: 839996.5198466986\n",
      "iteration 537 Training Accuracy: 0.589148\n",
      "-------------------------------------------------------------------\n",
      "iteration 538 Cost: 839996.512767417\n",
      "iteration 538 Training Accuracy: 0.589162\n",
      "-------------------------------------------------------------------\n",
      "iteration 539 Cost: 839996.5059983508\n",
      "iteration 539 Training Accuracy: 0.58915\n",
      "-------------------------------------------------------------------\n",
      "iteration 540 Cost: 839996.4997705455\n",
      "iteration 540 Training Accuracy: 0.589156\n",
      "-------------------------------------------------------------------\n",
      "iteration 541 Cost: 839996.4951810954\n",
      "iteration 541 Training Accuracy: 0.589156\n",
      "-------------------------------------------------------------------\n",
      "iteration 542 Cost: 839996.4905801653\n",
      "iteration 542 Training Accuracy: 0.589172\n",
      "-------------------------------------------------------------------\n",
      "iteration 543 Cost: 839996.4829693137\n",
      "iteration 543 Training Accuracy: 0.589172\n",
      "-------------------------------------------------------------------\n",
      "iteration 544 Cost: 839996.4769144806\n",
      "iteration 544 Training Accuracy: 0.589168\n",
      "-------------------------------------------------------------------\n",
      "iteration 545 Cost: 839996.4714019197\n",
      "iteration 545 Training Accuracy: 0.58915\n",
      "-------------------------------------------------------------------\n",
      "iteration 546 Cost: 839996.4672763204\n",
      "iteration 546 Training Accuracy: 0.589148\n",
      "-------------------------------------------------------------------\n",
      "iteration 547 Cost: 839996.462633654\n",
      "iteration 547 Training Accuracy: 0.589174\n",
      "-------------------------------------------------------------------\n",
      "iteration 548 Cost: 839996.4544417448\n",
      "iteration 548 Training Accuracy: 0.58918\n",
      "-------------------------------------------------------------------\n",
      "iteration 549 Cost: 839996.4513278741\n",
      "iteration 549 Training Accuracy: 0.589164\n",
      "-------------------------------------------------------------------\n",
      "iteration 550 Cost: 839996.447679914\n",
      "iteration 550 Training Accuracy: 0.58916\n",
      "-------------------------------------------------------------------\n",
      "iteration 551 Cost: 839996.442956664\n",
      "iteration 551 Training Accuracy: 0.589154\n",
      "-------------------------------------------------------------------\n",
      "iteration 552 Cost: 839996.4396989606\n",
      "iteration 552 Training Accuracy: 0.589144\n",
      "-------------------------------------------------------------------\n",
      "iteration 553 Cost: 839996.435243748\n",
      "iteration 553 Training Accuracy: 0.589144\n",
      "-------------------------------------------------------------------\n",
      "iteration 554 Cost: 839996.4292282782\n",
      "iteration 554 Training Accuracy: 0.58916\n",
      "-------------------------------------------------------------------\n",
      "iteration 555 Cost: 839996.4262116816\n",
      "iteration 555 Training Accuracy: 0.589172\n",
      "-------------------------------------------------------------------\n",
      "iteration 556 Cost: 839996.4229734394\n",
      "iteration 556 Training Accuracy: 0.589182\n",
      "-------------------------------------------------------------------\n",
      "iteration 557 Cost: 839996.4194251668\n",
      "iteration 557 Training Accuracy: 0.589176\n",
      "-------------------------------------------------------------------\n",
      "iteration 558 Cost: 839996.4154164208\n",
      "iteration 558 Training Accuracy: 0.589176\n",
      "-------------------------------------------------------------------\n",
      "iteration 559 Cost: 839996.4092388535\n",
      "iteration 559 Training Accuracy: 0.589158\n",
      "-------------------------------------------------------------------\n",
      "iteration 560 Cost: 839996.4073301009\n",
      "iteration 560 Training Accuracy: 0.589156\n",
      "-------------------------------------------------------------------\n",
      "iteration 561 Cost: 839996.4045526587\n",
      "iteration 561 Training Accuracy: 0.589156\n",
      "-------------------------------------------------------------------\n",
      "iteration 562 Cost: 839996.4004238115\n",
      "iteration 562 Training Accuracy: 0.58916\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 563 Cost: 839996.3969821469\n",
      "iteration 563 Training Accuracy: 0.589164\n",
      "-------------------------------------------------------------------\n",
      "iteration 564 Cost: 839996.391252431\n",
      "iteration 564 Training Accuracy: 0.589168\n",
      "-------------------------------------------------------------------\n",
      "iteration 565 Cost: 839996.3896138205\n",
      "iteration 565 Training Accuracy: 0.58919\n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "res, f, d = sy.optimize.fmin_l_bfgs_b(cost, [0.0]*nItems, derivative, args=[0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grad': array([ 0.00367678, -0.0169391 ,  0.01158269, ..., -0.00479811,\n",
       "         0.        , -0.00378774]),\n",
       " 'task': b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH',\n",
       " 'funcalls': 565,\n",
       " 'nit': 552,\n",
       " 'warnflag': 0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unpack(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions, labels = generate_outputs(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5771808701453179"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Latent Factor Model with Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple non-biased latent factor model that is wrapped into a binary function (sigmoid function) as a base line model, using popularity as the item's sole feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itemBiases = defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each user and item we now have a low dimensional descriptor (representing that user's preferences, and that item's properties), of dimension K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userGamma = {}\n",
    "itemGamma = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for u in itemsPerUser:\n",
    "    userGamma[u] = [random.random() * 0.1 - 0.05 for k in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in usersPerItem:\n",
    "    itemGamma[i] = [random.random() * 0.1 - 0.05 for k in range(K)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use another library in this example to perform gradient descent. This library requires that we pass it a \"flat\" parameter vector (theta) containing all of our parameters. This utility function just converts between a flat feature vector, and our model parameters, i.e., it \"unpacks\" theta into our offset and bias parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global itemBiases\n",
    "    global userGamma\n",
    "    global itemGamma\n",
    "    index = 0\n",
    "    itemBiases = dict(zip(items, theta[0:index + nItems]))\n",
    "    index += nItems\n",
    "    for u in users:\n",
    "        userGamma[u] = theta[index:index + K]\n",
    "        index += K\n",
    "    for i in items:\n",
    "        itemGamma[i] = theta[index:index + K]\n",
    "        index += K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "f(u, i, j) = \\gamma_u \\gamma_i + \\beta_i - (\\gamma_u \\gamma_j + \\beta_j)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "p(i >_u j) = \\sigma(f(u, i, j))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(user, item_i, item_j):\n",
    "    return inner(userGamma[user], itemGamma[item_i]) + itemBiases[item_i] - (inner(userGamma[user], itemGamma[item_j]) + itemBiases[item_j]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\text{Cost Function (arg min)}:= \\sum_{u,i,j} -\\ln(\\sigma(\\gamma_u \\gamma_i + \\beta_i - (\\gamma_u \\gamma_j + \\beta_j))) + \\lambda [ \\sum_u \\beta_u^2 + \\sum_i \\beta_i^2 + \\sum_i ||\\gamma_i||_2^2 + \\sum_u ||\\gamma_u||_2^2 ]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\sum_{u,i,j} \\ln(\\sigma(\\gamma_u \\gamma_i + \\beta_i - (\\gamma_u \\gamma_j + \\beta_j))) = \\sum_{u,i,j} ln\\left( \\frac{1}{1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}} \\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(theta, lamb):\n",
    "    unpack(theta)\n",
    "    cost = 0.0\n",
    "    predictions = []\n",
    "    for u, i, j in train_data:\n",
    "        x = prediction(u, i, j)\n",
    "        predictions.append(sigmoid(x))\n",
    "        cost += np.log(sigmoid(x))\n",
    "        \n",
    "    for u in users:\n",
    "        for k in range(K):\n",
    "            cost -= lamb*userGamma[u][k]**2\n",
    "    for i in items:\n",
    "        cost -= lamb*itemBiases[i]**2\n",
    "        for k in range(K):\n",
    "            cost -= lamb*itemGamma[i][k]**2\n",
    "        \n",
    "    print('Current Cost: %s' % -cost)\n",
    "    print('Current Training Accuracy: %s' % accuracy(predictions, train_labels))\n",
    "    print('-------------------------------------------------------------------')\n",
    "        \n",
    "    return -cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\gamma_{u,k}} = \\frac{(\\gamma_{j,k} - \\gamma_{i,k}) \\cdot e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}{1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}} + 2 \\lambda \\gamma_{i,k} \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\gamma_{i,k}} = -\\frac{\\gamma_{u,k} \\cdot e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}{1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}} + 2 \\lambda \\gamma_{i,k}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\gamma_{j,k}} = \\frac{\\gamma_{u,k} \\cdot e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}{1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}} + 2 \\lambda \\gamma_{j,k}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\beta_i} = -\\frac{e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}{1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}} + 2 \\lambda \\beta_i\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial f}{\\partial \\beta_j} = \\frac{e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}}{1 + e^{\\gamma_u \\gamma_j + \\beta_j - (\\gamma_u \\gamma_i + \\beta_i)}} + 2 \\lambda \\beta_j\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def derivative(theta, lamb):\n",
    "    unpack(theta)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    dUserGamma = {}\n",
    "    dItemGamma = {}\n",
    "    for u in users:\n",
    "        dUserGamma[u] = [0.0 for k in range(K)]\n",
    "    for i in items:\n",
    "        dItemGamma[i] = [0.0 for k in range(K)]\n",
    "    for u, i, j in train_data:\n",
    "        x = prediction(u, i ,j)\n",
    "        dbase = 1 / (1 + np.exp(x))\n",
    "        dItemBiases[i] += -dbase\n",
    "        dItemBiases[j] += dbase\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += (itemGamma[j][k] - itemGamma[i][k]) * dbase\n",
    "            dItemGamma_k = userGamma[u][k] * dbase\n",
    "            dItemGamma[i][k] += -dItemGamma_k\n",
    "            dItemGamma[j][k] += dItemGamma_k\n",
    "    for u in userGamma:\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += 2*lamb*userGamma[u][k]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "        for k in range(K):\n",
    "            dItemGamma[i][k] += 2*lamb*itemGamma[i][k]\n",
    "    dtheta = [dItemBiases[i] for i in items]\n",
    "    for u in users:\n",
    "        dtheta += dUserGamma[u]\n",
    "    for i in items:\n",
    "        dtheta += dItemGamma[i]\n",
    "    return np.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Cost: 881091.0857679009\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 870264.9239619058\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 850367.2754800008\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 848066.2511377814\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 846469.3843631307\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 845704.1224281399\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 845070.0794451325\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 844755.0600726622\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 844353.3228248961\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 843923.5966478076\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 843523.2137851532\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 843261.1812826748\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 842851.1475617574\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 842536.2539052817\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 842127.8787732864\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 841105.6168217745\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 840078.5088940739\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 834009.5293447216\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 829269.9146106127\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 812388.2507772259\n",
      "Current Training Accuracy: 0.0\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 755298.4580966914\n",
      "Current Training Accuracy: 7.70961499126768e-05\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 737890.069219023\n",
      "Current Training Accuracy: 7.866954072722124e-06\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 675797.3964874501\n",
      "Current Training Accuracy: 9.361675346539327e-05\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 636161.0790147551\n",
      "Current Training Accuracy: 0.00013688500086536494\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 574208.5771173697\n",
      "Current Training Accuracy: 0.00023364853595984707\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 549930.5897424254\n",
      "Current Training Accuracy: 0.0002226348002580361\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 513714.79829284863\n",
      "Current Training Accuracy: 0.0004413361234797111\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 490094.9525983087\n",
      "Current Training Accuracy: 0.0004122283934106393\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 457745.31199458905\n",
      "Current Training Accuracy: 0.0006348631936686753\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 432620.524076866\n",
      "Current Training Accuracy: 0.0015442830844753527\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 409501.54758297664\n",
      "Current Training Accuracy: 0.0023522192677439147\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 392261.28753391677\n",
      "Current Training Accuracy: 0.002897399184983558\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 373106.13718831236\n",
      "Current Training Accuracy: 0.003701401891215759\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 358180.0058900078\n",
      "Current Training Accuracy: 0.0049577544566294825\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 342788.88779688993\n",
      "Current Training Accuracy: 0.005894708686690687\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 329933.66384713846\n",
      "Current Training Accuracy: 0.006844250043268247\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 318981.3155816493\n",
      "Current Training Accuracy: 0.008362572179303618\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 308783.6710088132\n",
      "Current Training Accuracy: 0.010149157449218812\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 298992.7541752898\n",
      "Current Training Accuracy: 0.011702094183174158\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 290687.78871002246\n",
      "Current Training Accuracy: 0.013097691835675064\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 283676.46062895574\n",
      "Current Training Accuracy: 0.014619947448746794\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 276953.29271487205\n",
      "Current Training Accuracy: 0.016409679500291076\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 270907.74385795905\n",
      "Current Training Accuracy: 0.018304828736409838\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 265554.67511615914\n",
      "Current Training Accuracy: 0.02010950800069229\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 260459.16309780258\n",
      "Current Training Accuracy: 0.021782022436553014\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 255517.12915196893\n",
      "Current Training Accuracy: 0.02392026055351889\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 251191.86756843157\n",
      "Current Training Accuracy: 0.025963308526204824\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 246975.75799071102\n",
      "Current Training Accuracy: 0.027465110058687476\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 243058.49786183157\n",
      "Current Training Accuracy: 0.029106943373664585\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 239506.59678165012\n",
      "Current Training Accuracy: 0.031548845917837534\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 236133.95606965345\n",
      "Current Training Accuracy: 0.033281149204650945\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 232738.97864295464\n",
      "Current Training Accuracy: 0.0348734207089699\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 229817.07098156735\n",
      "Current Training Accuracy: 0.03713123652784115\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 227062.70136949193\n",
      "Current Training Accuracy: 0.03873530846326919\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 224315.8940477458\n",
      "Current Training Accuracy: 0.04028195163396636\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 221814.5438378837\n",
      "Current Training Accuracy: 0.04233601334235411\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 219445.42224475747\n",
      "Current Training Accuracy: 0.04400144751954938\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 217022.42390481159\n",
      "Current Training Accuracy: 0.04565508126563557\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Cost: 214585.6889601349\n",
      "Current Training Accuracy: 0.04800336705634312\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 212615.76911122346\n",
      "Current Training Accuracy: 0.04963812011265478\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 210446.8919273968\n",
      "Current Training Accuracy: 0.05074814733231588\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 208161.6015739953\n",
      "Current Training Accuracy: 0.05348270056799408\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 206322.78371357266\n",
      "Current Training Accuracy: 0.05554462923045455\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 204617.7470889815\n",
      "Current Training Accuracy: 0.05664285601900656\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 202859.9973816936\n",
      "Current Training Accuracy: 0.05832087732271819\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 201215.3589649264\n",
      "Current Training Accuracy: 0.060554305583964\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 199409.43435138787\n",
      "Current Training Accuracy: 0.06261780763723901\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 197756.698735905\n",
      "Current Training Accuracy: 0.06450666331009959\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 196264.8641757888\n",
      "Current Training Accuracy: 0.06617209748729487\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 194827.70491034637\n",
      "Current Training Accuracy: 0.067496105857734\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 193201.80190293991\n",
      "Current Training Accuracy: 0.06979718992400523\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 192006.81697153932\n",
      "Current Training Accuracy: 0.07233349591705084\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 190649.35478214227\n",
      "Current Training Accuracy: 0.07315165914061393\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 189421.46938363256\n",
      "Current Training Accuracy: 0.07433406233774408\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 188157.31852867894\n",
      "Current Training Accuracy: 0.07684676746857153\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 187030.62585164173\n",
      "Current Training Accuracy: 0.07872696949195211\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 185877.95301135164\n",
      "Current Training Accuracy: 0.07992117312019133\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 184763.69978327767\n",
      "Current Training Accuracy: 0.08158424721116478\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 183814.80269066626\n",
      "Current Training Accuracy: 0.0831096496058656\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 182773.49308502467\n",
      "Current Training Accuracy: 0.08448636656859197\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 181731.794544988\n",
      "Current Training Accuracy: 0.08633038060323804\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 180804.69608131368\n",
      "Current Training Accuracy: 0.08805167015434964\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 179871.92619807128\n",
      "Current Training Accuracy: 0.08920968579385434\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 178834.5116004152\n",
      "Current Training Accuracy: 0.09092940195415139\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 178028.3201930254\n",
      "Current Training Accuracy: 0.09258067561401577\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 177123.5312004612\n",
      "Current Training Accuracy: 0.09423509605550923\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 176215.18936405063\n",
      "Current Training Accuracy: 0.09655348742074044\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 175306.06040924694\n",
      "Current Training Accuracy: 0.09826848340859386\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 174493.79738218643\n",
      "Current Training Accuracy: 0.09971836304419655\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 173660.49656169384\n",
      "Current Training Accuracy: 0.10148528092892993\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 172963.31263801898\n",
      "Current Training Accuracy: 0.1033009739289142\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 172258.776498438\n",
      "Current Training Accuracy: 0.10464936985697877\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 171441.46028955726\n",
      "Current Training Accuracy: 0.10682143587645736\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 170843.6044674678\n",
      "Current Training Accuracy: 0.10944427836430291\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 170103.50587799354\n",
      "Current Training Accuracy: 0.11017275831143698\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 169200.32701812199\n",
      "Current Training Accuracy: 0.11205925389807574\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 168570.0021458746\n",
      "Current Training Accuracy: 0.11424548043488522\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 167952.7626884233\n",
      "Current Training Accuracy: 0.11561590383435341\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 167335.88695174607\n",
      "Current Training Accuracy: 0.11741586292619224\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 166662.6395171042\n",
      "Current Training Accuracy: 0.11994823544220148\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 165979.4352345599\n",
      "Current Training Accuracy: 0.12181978381610208\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 165267.85151579496\n",
      "Current Training Accuracy: 0.12359535535031546\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 164562.1349163136\n",
      "Current Training Accuracy: 0.12607737936025928\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 163909.39967065118\n",
      "Current Training Accuracy: 0.12835250247809055\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 163397.24101467777\n",
      "Current Training Accuracy: 0.1300352439542458\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 162731.96402718627\n",
      "Current Training Accuracy: 0.13154805922243026\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 162189.75702150638\n",
      "Current Training Accuracy: 0.13377440722501063\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 161643.28382315117\n",
      "Current Training Accuracy: 0.13592601916390012\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 161061.16752287175\n",
      "Current Training Accuracy: 0.13787230360149158\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 160526.0141687855\n",
      "Current Training Accuracy: 0.14012854602954827\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 159979.94367174874\n",
      "Current Training Accuracy: 0.14182859480466353\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 159381.2163868542\n",
      "Current Training Accuracy: 0.14385669556461128\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 158869.72882053012\n",
      "Current Training Accuracy: 0.14648740500652957\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 158353.88277975318\n",
      "Current Training Accuracy: 0.14826218984533568\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Cost: 157790.70242607562\n",
      "Current Training Accuracy: 0.15056484730242145\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 157323.6199151246\n",
      "Current Training Accuracy: 0.15336469625690324\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 156859.50273562822\n",
      "Current Training Accuracy: 0.15471230548956055\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 156414.65805148546\n",
      "Current Training Accuracy: 0.15572635586953443\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 155960.39530420417\n",
      "Current Training Accuracy: 0.15830120993753638\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 155497.10646791809\n",
      "Current Training Accuracy: 0.16033953773777868\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 155048.17027376746\n",
      "Current Training Accuracy: 0.1619672105354249\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 154633.13428460728\n",
      "Current Training Accuracy: 0.1642769482511761\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 154246.69014357575\n",
      "Current Training Accuracy: 0.16610444168226945\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 153877.94471838296\n",
      "Current Training Accuracy: 0.1674221564894504\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 153505.7324849363\n",
      "Current Training Accuracy: 0.16935428040971096\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 153120.5854021823\n",
      "Current Training Accuracy: 0.17122818886983338\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 152722.29912751942\n",
      "Current Training Accuracy: 0.17339081454442468\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 152320.64562215706\n",
      "Current Training Accuracy: 0.17553849300627783\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 151900.41339951658\n",
      "Current Training Accuracy: 0.17727158298849852\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 151554.61637024718\n",
      "Current Training Accuracy: 0.17860424500841765\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 151209.47511659676\n",
      "Current Training Accuracy: 0.17965133659549695\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 150892.4411016175\n",
      "Current Training Accuracy: 0.18127428922069952\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 150567.69075072967\n",
      "Current Training Accuracy: 0.18335116509589816\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 150219.23874829727\n",
      "Current Training Accuracy: 0.18513460358418427\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 149802.28261189902\n",
      "Current Training Accuracy: 0.18763314819768082\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 149468.94057930316\n",
      "Current Training Accuracy: 0.18909640165520714\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 149167.8906560353\n",
      "Current Training Accuracy: 0.1898162279528612\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 148832.90771957385\n",
      "Current Training Accuracy: 0.19108516764479128\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 148543.65722765677\n",
      "Current Training Accuracy: 0.19237141463568136\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 148292.4744930861\n",
      "Current Training Accuracy: 0.19382601444372768\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 147951.9326616863\n",
      "Current Training Accuracy: 0.19572431046147554\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 147729.68094637428\n",
      "Current Training Accuracy: 0.19820712116682662\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 147375.9720657076\n",
      "Current Training Accuracy: 0.19853438645625188\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 147046.05405238783\n",
      "Current Training Accuracy: 0.19952640936482213\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 146766.5593621714\n",
      "Current Training Accuracy: 0.20168667495319162\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 146509.33053771884\n",
      "Current Training Accuracy: 0.20301383010525983\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 146286.19676796603\n",
      "Current Training Accuracy: 0.20428827666504082\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 146003.1266421783\n",
      "Current Training Accuracy: 0.20727771921267524\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 145726.60328420156\n",
      "Current Training Accuracy: 0.209138253850874\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 145455.45660418592\n",
      "Current Training Accuracy: 0.2100704879084916\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 145089.71234723143\n",
      "Current Training Accuracy: 0.21233145050899194\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 144862.42407937875\n",
      "Current Training Accuracy: 0.21505735009519014\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 144628.90814419516\n",
      "Current Training Accuracy: 0.21565523860471703\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 144314.39529485346\n",
      "Current Training Accuracy: 0.21728212470695596\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 144088.14979008434\n",
      "Current Training Accuracy: 0.21940541561118365\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 143875.9246329548\n",
      "Current Training Accuracy: 0.22087732271818997\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 143610.48855042024\n",
      "Current Training Accuracy: 0.2227229101436506\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 143382.62331968694\n",
      "Current Training Accuracy: 0.2249335242380855\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 143139.97984476402\n",
      "Current Training Accuracy: 0.22540475478704156\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 142872.7110879452\n",
      "Current Training Accuracy: 0.22649826140314994\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 142663.2392871845\n",
      "Current Training Accuracy: 0.2280220904070362\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 142455.32946004195\n",
      "Current Training Accuracy: 0.22943263527227528\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 142174.91380161565\n",
      "Current Training Accuracy: 0.23060638481992543\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 141914.76306910996\n",
      "Current Training Accuracy: 0.23260931132684048\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 141693.24760337657\n",
      "Current Training Accuracy: 0.23389477162232328\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 141477.41483814002\n",
      "Current Training Accuracy: 0.23444703179822837\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 141244.1159575476\n",
      "Current Training Accuracy: 0.2360511037336564\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 141015.89024673955\n",
      "Current Training Accuracy: 0.2377503658133644\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 140828.79911794848\n",
      "Current Training Accuracy: 0.23928048838050883\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 140664.75007030403\n",
      "Current Training Accuracy: 0.24002548893119563\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Cost: 140465.22484599275\n",
      "Current Training Accuracy: 0.24099863114999134\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 140235.35087775375\n",
      "Current Training Accuracy: 0.24310225466903723\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 140020.4471172846\n",
      "Current Training Accuracy: 0.24516654341771954\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 139842.86850352093\n",
      "Current Training Accuracy: 0.24548515505766477\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 139658.31452834595\n",
      "Current Training Accuracy: 0.24599021350913353\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 139518.28004754838\n",
      "Current Training Accuracy: 0.24686108532498388\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 139348.6047683453\n",
      "Current Training Accuracy: 0.24810799754551033\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 139167.58889400464\n",
      "Current Training Accuracy: 0.2490850732413424\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 138956.4907148725\n",
      "Current Training Accuracy: 0.2504728039397706\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 138768.05637606294\n",
      "Current Training Accuracy: 0.25150966848655537\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 138591.73648288453\n",
      "Current Training Accuracy: 0.25261261544755104\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 138412.88131810093\n",
      "Current Training Accuracy: 0.25373758987995026\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 138236.4940728607\n",
      "Current Training Accuracy: 0.25556429661563634\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 138070.77059404989\n",
      "Current Training Accuracy: 0.25629828343062133\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 137907.4983698909\n",
      "Current Training Accuracy: 0.2569732680900609\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 137729.28593941903\n",
      "Current Training Accuracy: 0.25831851723649635\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 137551.26923077775\n",
      "Current Training Accuracy: 0.2598730273612663\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 137362.88499126778\n",
      "Current Training Accuracy: 0.26133156064634894\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 137187.25465413943\n",
      "Current Training Accuracy: 0.2629694604842897\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 137048.82039597657\n",
      "Current Training Accuracy: 0.26407712761772895\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 136888.69434922637\n",
      "Current Training Accuracy: 0.2648119011281212\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 136693.1661476932\n",
      "Current Training Accuracy: 0.26646946835124374\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 136514.92858006936\n",
      "Current Training Accuracy: 0.2679138411189955\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 136328.039697461\n",
      "Current Training Accuracy: 0.26984203156221975\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 136172.40251668871\n",
      "Current Training Accuracy: 0.27101342102364806\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 136034.54477553113\n",
      "Current Training Accuracy: 0.27162547005050586\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 135870.48762651064\n",
      "Current Training Accuracy: 0.27264424060292336\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 135709.82986418373\n",
      "Current Training Accuracy: 0.2750373680318454\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 135564.68453569314\n",
      "Current Training Accuracy: 0.2757202196453577\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 135440.34042905064\n",
      "Current Training Accuracy: 0.2761450351652847\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 135256.4496019177\n",
      "Current Training Accuracy: 0.2778859921015781\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 135135.82231192416\n",
      "Current Training Accuracy: 0.2795238919395189\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 134995.69389245397\n",
      "Current Training Accuracy: 0.2799612945859622\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 134809.7409833552\n",
      "Current Training Accuracy: 0.2816770772692229\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 134687.30802924032\n",
      "Current Training Accuracy: 0.2826911276491968\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 134589.13697249812\n",
      "Current Training Accuracy: 0.2833220573658291\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 134477.6189408749\n",
      "Current Training Accuracy: 0.2849772645027298\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 134323.7602562769\n",
      "Current Training Accuracy: 0.28565932942083483\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 134192.94343110532\n",
      "Current Training Accuracy: 0.28676856994508865\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 134083.6375318144\n",
      "Current Training Accuracy: 0.287364885063801\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 133945.49185550434\n",
      "Current Training Accuracy: 0.2889610900451563\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 133832.8111165289\n",
      "Current Training Accuracy: 0.2904204100256463\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 133700.3560429549\n",
      "Current Training Accuracy: 0.2907445285334424\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 133536.91330531886\n",
      "Current Training Accuracy: 0.29173733813742\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 133444.13986034214\n",
      "Current Training Accuracy: 0.29299369070283365\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 133325.73495732163\n",
      "Current Training Accuracy: 0.2939550324905203\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 133222.2488321223\n",
      "Current Training Accuracy: 0.2945946158566326\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 133094.07644896224\n",
      "Current Training Accuracy: 0.29571172333495915\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 132992.71394253624\n",
      "Current Training Accuracy: 0.29653303334015135\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 132834.30618712737\n",
      "Current Training Accuracy: 0.29804899538996493\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 132701.2824526123\n",
      "Current Training Accuracy: 0.29929748100130593\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 132593.77885343708\n",
      "Current Training Accuracy: 0.29979309910788743\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 132470.9898832951\n",
      "Current Training Accuracy: 0.3009707821325739\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 132345.32426883583\n",
      "Current Training Accuracy: 0.3020697956165332\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 132253.93848137176\n",
      "Current Training Accuracy: 0.30313734128420156\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 132106.1320973402\n",
      "Current Training Accuracy: 0.3046344226442406\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Cost: 131992.40724266198\n",
      "Current Training Accuracy: 0.3064493289488176\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 131880.1064612784\n",
      "Current Training Accuracy: 0.30666252340418837\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 131762.49339397732\n",
      "Current Training Accuracy: 0.3068332363075664\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 131654.82704649473\n",
      "Current Training Accuracy: 0.3078040184401403\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 131544.30730660938\n",
      "Current Training Accuracy: 0.3094364114102302\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 131414.08862221168\n",
      "Current Training Accuracy: 0.31036313859999687\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 131274.96213636955\n",
      "Current Training Accuracy: 0.31167691993014146\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 131166.50457928833\n",
      "Current Training Accuracy: 0.31211274918577026\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 131086.7441364836\n",
      "Current Training Accuracy: 0.3127995342763189\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 130970.32515121966\n",
      "Current Training Accuracy: 0.3141801847160816\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 130868.62958172624\n",
      "Current Training Accuracy: 0.31534606730965903\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 130770.65346905345\n",
      "Current Training Accuracy: 0.3160155451012477\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 130649.05995284396\n",
      "Current Training Accuracy: 0.31674717183001083\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 130550.58126647824\n",
      "Current Training Accuracy: 0.31778246298598106\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 130455.02639456282\n",
      "Current Training Accuracy: 0.31886574256179495\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 130366.0856963939\n",
      "Current Training Accuracy: 0.3196146765895181\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 130271.03603338644\n",
      "Current Training Accuracy: 0.32006859983951413\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 130180.41010312347\n",
      "Current Training Accuracy: 0.3209780197303208\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 130056.32855549688\n",
      "Current Training Accuracy: 0.3218835061440911\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 129970.72547625781\n",
      "Current Training Accuracy: 0.322559277498938\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 129887.97039129722\n",
      "Current Training Accuracy: 0.32310681750239945\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 129790.13475523396\n",
      "Current Training Accuracy: 0.32405714555438425\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 129699.02577447883\n",
      "Current Training Accuracy: 0.3249516182324528\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 129606.42838491927\n",
      "Current Training Accuracy: 0.32552511918435423\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 129520.55753886742\n",
      "Current Training Accuracy: 0.3262496656544519\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 129436.91429939761\n",
      "Current Training Accuracy: 0.3269490378715169\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 129353.4501193115\n",
      "Current Training Accuracy: 0.3277616942272291\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 129258.87518775208\n",
      "Current Training Accuracy: 0.32914470475321367\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 129162.94725833097\n",
      "Current Training Accuracy: 0.32960964173891155\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 129081.90361369886\n",
      "Current Training Accuracy: 0.3301626886102239\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 129001.21733185026\n",
      "Current Training Accuracy: 0.33090847585631794\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 128925.99357972408\n",
      "Current Training Accuracy: 0.3316597699702629\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 128833.72369630076\n",
      "Current Training Accuracy: 0.3325070409238951\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 128733.01087457618\n",
      "Current Training Accuracy: 0.3335446921660871\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 128644.45817325059\n",
      "Current Training Accuracy: 0.3343306008779521\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 128567.35862805393\n",
      "Current Training Accuracy: 0.33461223783375554\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 128482.18901010203\n",
      "Current Training Accuracy: 0.3353769057696241\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 128407.18634887136\n",
      "Current Training Accuracy: 0.33614550718252906\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 128335.53224465439\n",
      "Current Training Accuracy: 0.33675991629560864\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 128245.4157910582\n",
      "Current Training Accuracy: 0.33773463190521896\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 128178.70025977814\n",
      "Current Training Accuracy: 0.33843164403606213\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 128107.59634826175\n",
      "Current Training Accuracy: 0.33874710889437826\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 128024.47344240965\n",
      "Current Training Accuracy: 0.33920339223059615\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 127961.62339140712\n",
      "Current Training Accuracy: 0.3398539893324103\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 127888.07027813353\n",
      "Current Training Accuracy: 0.3404872791352644\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 127787.27438991613\n",
      "Current Training Accuracy: 0.3419285051213871\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 127715.94219736273\n",
      "Current Training Accuracy: 0.3428347782305647\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 127644.7573382424\n",
      "Current Training Accuracy: 0.3431746306465063\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 127566.50080666157\n",
      "Current Training Accuracy: 0.34372531743159684\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 127498.23824701805\n",
      "Current Training Accuracy: 0.3445993360290763\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 127422.29393251256\n",
      "Current Training Accuracy: 0.3454529005459666\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 127344.75917240755\n",
      "Current Training Accuracy: 0.34616879336658435\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 127269.32130570672\n",
      "Current Training Accuracy: 0.34720329782714726\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 127203.80288517101\n",
      "Current Training Accuracy: 0.3474204257595544\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 127144.60600038379\n",
      "Current Training Accuracy: 0.34767138159447425\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 127069.24834828514\n",
      "Current Training Accuracy: 0.34849269159966645\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Cost: 127002.52402127843\n",
      "Current Training Accuracy: 0.3493423226395204\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 126935.14864846798\n",
      "Current Training Accuracy: 0.34967430810138933\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 126849.11319683799\n",
      "Current Training Accuracy: 0.3505947417278978\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 126778.9926917585\n",
      "Current Training Accuracy: 0.3515254023947008\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 126710.28299200063\n",
      "Current Training Accuracy: 0.35199977972528596\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 126641.82021967195\n",
      "Current Training Accuracy: 0.35284075711566\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 126576.85716294058\n",
      "Current Training Accuracy: 0.3531208206806489\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 126510.27551501998\n",
      "Current Training Accuracy: 0.35357238384442313\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 126463.85611561027\n",
      "Current Training Accuracy: 0.3540365341347137\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 126393.37867668188\n",
      "Current Training Accuracy: 0.3552126437685857\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 126334.66877356985\n",
      "Current Training Accuracy: 0.355826266186258\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 126278.87234064571\n",
      "Current Training Accuracy: 0.3558183992321853\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 126212.85113798623\n",
      "Current Training Accuracy: 0.35662082854760296\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 126122.58324706805\n",
      "Current Training Accuracy: 0.357092059096559\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 126086.081572201\n",
      "Current Training Accuracy: 0.35749563384048966\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 126041.43975321192\n",
      "Current Training Accuracy: 0.35789448841197663\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 125972.8272602259\n",
      "Current Training Accuracy: 0.3592940195415139\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 125890.74636640125\n",
      "Current Training Accuracy: 0.35964173891152823\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 125849.41035406811\n",
      "Current Training Accuracy: 0.35972198184307\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 125780.88331947394\n",
      "Current Training Accuracy: 0.36043708796828045\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 125726.6210851547\n",
      "Current Training Accuracy: 0.3609201189483456\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 125664.62244690808\n",
      "Current Training Accuracy: 0.36169108044747234\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 125599.04326226338\n",
      "Current Training Accuracy: 0.3624132668313482\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 125525.93419294133\n",
      "Current Training Accuracy: 0.36332976698082037\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 125471.97326539457\n",
      "Current Training Accuracy: 0.3635799361203329\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 125408.24991093387\n",
      "Current Training Accuracy: 0.3637648095410419\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 125346.37021615184\n",
      "Current Training Accuracy: 0.36438944569441606\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 125297.05310371157\n",
      "Current Training Accuracy: 0.3646050002360086\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 125249.20868342416\n",
      "Current Training Accuracy: 0.3649700269049829\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 125185.75429820741\n",
      "Current Training Accuracy: 0.36575672231225514\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 125159.4686573421\n",
      "Current Training Accuracy: 0.3667912267728181\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 125082.69942097612\n",
      "Current Training Accuracy: 0.36685730918702897\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 125038.3434939516\n",
      "Current Training Accuracy: 0.36693676542316345\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124998.72593140858\n",
      "Current Training Accuracy: 0.3671531066601633\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124937.37356262936\n",
      "Current Training Accuracy: 0.36760545651934484\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124879.68966132232\n",
      "Current Training Accuracy: 0.3680342055163082\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124807.89023735987\n",
      "Current Training Accuracy: 0.3688940635964567\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124754.55650069144\n",
      "Current Training Accuracy: 0.3691804207247038\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124718.24294132432\n",
      "Current Training Accuracy: 0.36922762244914015\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124663.31181471999\n",
      "Current Training Accuracy: 0.36951240618657266\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124613.86490051342\n",
      "Current Training Accuracy: 0.37002847837374325\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124556.93686117009\n",
      "Current Training Accuracy: 0.37060984627971744\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124509.62389952547\n",
      "Current Training Accuracy: 0.3711644665418443\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124456.8875170274\n",
      "Current Training Accuracy: 0.3716655915162767\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124406.17693950771\n",
      "Current Training Accuracy: 0.371958242207782\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124343.92981853662\n",
      "Current Training Accuracy: 0.3723358560032727\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124292.5450236631\n",
      "Current Training Accuracy: 0.3728582217537014\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124238.79626286982\n",
      "Current Training Accuracy: 0.3734317227056028\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124183.94939803307\n",
      "Current Training Accuracy: 0.37384237770819895\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124130.22173763686\n",
      "Current Training Accuracy: 0.3744347593498749\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124077.59129809348\n",
      "Current Training Accuracy: 0.3746440203282093\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 124021.50511430438\n",
      "Current Training Accuracy: 0.37496577874978365\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123978.75990615787\n",
      "Current Training Accuracy: 0.3752843903897289\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123930.26571581789\n",
      "Current Training Accuracy: 0.37569661878313954\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123880.09964289168\n",
      "Current Training Accuracy: 0.37617650298157557\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123831.42385063575\n",
      "Current Training Accuracy: 0.3764990480985572\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Cost: 123790.65515251078\n",
      "Current Training Accuracy: 0.37669021508252437\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123731.30581171911\n",
      "Current Training Accuracy: 0.37712919111978227\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123674.637360792\n",
      "Current Training Accuracy: 0.3776279560079928\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123623.7454327947\n",
      "Current Training Accuracy: 0.3781094135972434\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123577.35951931725\n",
      "Current Training Accuracy: 0.37851220164576677\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123531.96353491879\n",
      "Current Training Accuracy: 0.3787466368771339\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123479.56057153417\n",
      "Current Training Accuracy: 0.37933351165095897\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123423.87024661161\n",
      "Current Training Accuracy: 0.3796127885205406\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123390.19560208803\n",
      "Current Training Accuracy: 0.37974180656733325\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123344.41869096497\n",
      "Current Training Accuracy: 0.38010683323630756\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123299.08758290764\n",
      "Current Training Accuracy: 0.3805709835265982\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123258.62233123124\n",
      "Current Training Accuracy: 0.38096275783941974\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123216.84819728485\n",
      "Current Training Accuracy: 0.38131441068647043\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123167.10511492907\n",
      "Current Training Accuracy: 0.3818108154884592\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123118.50224311448\n",
      "Current Training Accuracy: 0.3820303035070881\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123078.87117609309\n",
      "Current Training Accuracy: 0.3821506679044008\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 123022.26445896093\n",
      "Current Training Accuracy: 0.3824724263259751\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122984.8468064724\n",
      "Current Training Accuracy: 0.38277687744858946\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122942.45904968865\n",
      "Current Training Accuracy: 0.3830766083987602\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122898.69848625726\n",
      "Current Training Accuracy: 0.38345658228047264\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122863.67252517364\n",
      "Current Training Accuracy: 0.3839647875135705\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122814.86824854808\n",
      "Current Training Accuracy: 0.38424170429693033\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122759.58644233442\n",
      "Current Training Accuracy: 0.3847538430070645\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122725.93953272502\n",
      "Current Training Accuracy: 0.3849182623471844\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122702.18450372542\n",
      "Current Training Accuracy: 0.3850000786695407\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122651.11999632005\n",
      "Current Training Accuracy: 0.3853076765737842\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122605.79591559057\n",
      "Current Training Accuracy: 0.3857765470365184\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122572.8319974442\n",
      "Current Training Accuracy: 0.3859519801123401\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122527.6937449491\n",
      "Current Training Accuracy: 0.38624227071762357\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122489.38632111701\n",
      "Current Training Accuracy: 0.3866434853753324\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122447.57019114729\n",
      "Current Training Accuracy: 0.38693613606683763\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122405.21441714792\n",
      "Current Training Accuracy: 0.3872555344021902\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122358.2404748284\n",
      "Current Training Accuracy: 0.3875489717891027\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122317.54408801405\n",
      "Current Training Accuracy: 0.3877613795490662\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122279.82188289006\n",
      "Current Training Accuracy: 0.3882507040923895\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122239.16992883672\n",
      "Current Training Accuracy: 0.38844816463961485\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122205.82867173798\n",
      "Current Training Accuracy: 0.38856302216907657\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122163.18225531388\n",
      "Current Training Accuracy: 0.38902481237314535\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122118.94213145513\n",
      "Current Training Accuracy: 0.3894677218874396\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122079.32295546483\n",
      "Current Training Accuracy: 0.38976194596975944\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122038.66740102759\n",
      "Current Training Accuracy: 0.39009786490866466\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 122000.90840548146\n",
      "Current Training Accuracy: 0.3904927860031153\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121960.02439434965\n",
      "Current Training Accuracy: 0.3907319414069261\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121923.73202351229\n",
      "Current Training Accuracy: 0.3908688264077914\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121882.99114604513\n",
      "Current Training Accuracy: 0.3911897981339585\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121846.14528986477\n",
      "Current Training Accuracy: 0.39175621882719447\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121804.9448355863\n",
      "Current Training Accuracy: 0.39198908066774707\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121751.81494342303\n",
      "Current Training Accuracy: 0.3924414305269286\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121717.16994491711\n",
      "Current Training Accuracy: 0.3927152005286593\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121677.57632054146\n",
      "Current Training Accuracy: 0.3930298786915682\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121632.51111641311\n",
      "Current Training Accuracy: 0.3934342401309061\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121595.90170935383\n",
      "Current Training Accuracy: 0.3935538178328115\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121555.91718738174\n",
      "Current Training Accuracy: 0.3937009298739714\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121512.01291021744\n",
      "Current Training Accuracy: 0.39392592476045124\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121468.76460149315\n",
      "Current Training Accuracy: 0.39443806347058546\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121430.51968496235\n",
      "Current Training Accuracy: 0.39477240901867616\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Cost: 121394.51866008584\n",
      "Current Training Accuracy: 0.39502965841685417\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121352.81161621706\n",
      "Current Training Accuracy: 0.39535928379250124\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121316.84509441983\n",
      "Current Training Accuracy: 0.39568340230029736\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121278.63360562982\n",
      "Current Training Accuracy: 0.3958580486807118\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121236.92871717046\n",
      "Current Training Accuracy: 0.39609799077992985\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121200.23270285508\n",
      "Current Training Accuracy: 0.396370187390846\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121154.54968619415\n",
      "Current Training Accuracy: 0.3967682552669258\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121114.84337353759\n",
      "Current Training Accuracy: 0.3972733137183945\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121078.5955300142\n",
      "Current Training Accuracy: 0.3973511965637145\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121043.0526049109\n",
      "Current Training Accuracy: 0.3974518935758453\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 121004.84635086499\n",
      "Current Training Accuracy: 0.39773117044542694\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120966.6386006672\n",
      "Current Training Accuracy: 0.3982810705351102\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120924.39427716583\n",
      "Current Training Accuracy: 0.3984541435247101\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120889.3411947119\n",
      "Current Training Accuracy: 0.39860518904290637\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120856.77335311123\n",
      "Current Training Accuracy: 0.39881523671664804\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120814.63558485926\n",
      "Current Training Accuracy: 0.39918891703510234\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120772.811126599\n",
      "Current Training Accuracy: 0.39958305143414574\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120734.93735301025\n",
      "Current Training Accuracy: 0.39975297764211654\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120705.37833182966\n",
      "Current Training Accuracy: 0.39981748666551287\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120671.15279245221\n",
      "Current Training Accuracy: 0.4000338279025127\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120646.14788905473\n",
      "Current Training Accuracy: 0.40023679531758893\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120618.92851109941\n",
      "Current Training Accuracy: 0.40045392324999607\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120575.41366219292\n",
      "Current Training Accuracy: 0.4009149267586576\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120540.40257329715\n",
      "Current Training Accuracy: 0.4012909671633337\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120505.87513729614\n",
      "Current Training Accuracy: 0.4013869440030209\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120460.3912963316\n",
      "Current Training Accuracy: 0.4016591406139371\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120433.04400835629\n",
      "Current Training Accuracy: 0.4018628947244206\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120393.14235416256\n",
      "Current Training Accuracy: 0.4021996003587331\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120351.26282611174\n",
      "Current Training Accuracy: 0.4027510738392309\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120314.93279488246\n",
      "Current Training Accuracy: 0.40282423651210725\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120282.85758808536\n",
      "Current Training Accuracy: 0.40294774769104896\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120250.58665393355\n",
      "Current Training Accuracy: 0.403179036140787\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120218.78437172365\n",
      "Current Training Accuracy: 0.403551929763834\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120184.73822391871\n",
      "Current Training Accuracy: 0.40389414226599746\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120147.77776252464\n",
      "Current Training Accuracy: 0.40436143933791713\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120111.91737638431\n",
      "Current Training Accuracy: 0.4046855578457133\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120080.43644435429\n",
      "Current Training Accuracy: 0.40486177761694225\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120046.14129449886\n",
      "Current Training Accuracy: 0.4050560913825385\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 120012.34079014989\n",
      "Current Training Accuracy: 0.405375489717891\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119979.20436458944\n",
      "Current Training Accuracy: 0.40566106015073083\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119944.13818231832\n",
      "Current Training Accuracy: 0.40585930739336346\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119905.49098932011\n",
      "Current Training Accuracy: 0.40619286624604684\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119872.48365322142\n",
      "Current Training Accuracy: 0.4064973173686612\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119840.51059872047\n",
      "Current Training Accuracy: 0.4065704800415375\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119790.5411000127\n",
      "Current Training Accuracy: 0.4068599839514137\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119764.46096211932\n",
      "Current Training Accuracy: 0.4071203801312208\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119736.3608258314\n",
      "Current Training Accuracy: 0.40730289346570797\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119696.02441951771\n",
      "Current Training Accuracy: 0.4076356656229841\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119670.2446637864\n",
      "Current Training Accuracy: 0.407808738612584\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119634.93561167667\n",
      "Current Training Accuracy: 0.4079975455103293\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119599.01359990913\n",
      "Current Training Accuracy: 0.4084506820649181\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119561.24740349984\n",
      "Current Training Accuracy: 0.40852463143320167\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119539.61389652081\n",
      "Current Training Accuracy: 0.40855924603112165\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119507.74464966022\n",
      "Current Training Accuracy: 0.4088102018660415\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119478.30307753064\n",
      "Current Training Accuracy: 0.4091956826156049\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119444.48622577012\n",
      "Current Training Accuracy: 0.40953474833613923\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Cost: 119408.91944006097\n",
      "Current Training Accuracy: 0.4097314221879573\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119368.77215906442\n",
      "Current Training Accuracy: 0.410013845839168\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119343.42400649682\n",
      "Current Training Accuracy: 0.4102899759271205\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119307.52469464536\n",
      "Current Training Accuracy: 0.4103788725081423\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119273.35026946485\n",
      "Current Training Accuracy: 0.4104921566467895\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119248.66766722997\n",
      "Current Training Accuracy: 0.41066129615935304\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119219.06088848779\n",
      "Current Training Accuracy: 0.4109075318218292\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119183.203577365\n",
      "Current Training Accuracy: 0.4111828752143745\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119149.96179057883\n",
      "Current Training Accuracy: 0.41138190915241435\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119121.67397923094\n",
      "Current Training Accuracy: 0.41144484478499616\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119083.86957313877\n",
      "Current Training Accuracy: 0.41160061047563606\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119055.15728682399\n",
      "Current Training Accuracy: 0.4118956212533631\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119025.10296383756\n",
      "Current Training Accuracy: 0.412060040593483\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 119003.11273813566\n",
      "Current Training Accuracy: 0.41220951272086476\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118973.64874180713\n",
      "Current Training Accuracy: 0.4124494548200828\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118948.8724309028\n",
      "Current Training Accuracy: 0.41262488789590446\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118925.82143037952\n",
      "Current Training Accuracy: 0.41263668832701356\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118876.13261363372\n",
      "Current Training Accuracy: 0.4128593231272716\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118855.7997431882\n",
      "Current Training Accuracy: 0.4131157858300423\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118832.00273707052\n",
      "Current Training Accuracy: 0.41317557468099503\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118800.90674490093\n",
      "Current Training Accuracy: 0.41339663609043853\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118780.18229413051\n",
      "Current Training Accuracy: 0.4135775760341111\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118752.55063125445\n",
      "Current Training Accuracy: 0.4138521327312491\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118726.3060600664\n",
      "Current Training Accuracy: 0.41419355853800527\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118695.4079223445\n",
      "Current Training Accuracy: 0.414210079141558\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118671.08435284228\n",
      "Current Training Accuracy: 0.41425334738895797\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118640.75988754554\n",
      "Current Training Accuracy: 0.4143886589990088\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118613.85570424127\n",
      "Current Training Accuracy: 0.4147049105527322\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118584.67989034601\n",
      "Current Training Accuracy: 0.41479774061079033\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118552.15921190126\n",
      "Current Training Accuracy: 0.4149448526519502\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118529.0148224257\n",
      "Current Training Accuracy: 0.4150660037446701\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118501.95191823045\n",
      "Current Training Accuracy: 0.4152485170791573\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118463.29170438065\n",
      "Current Training Accuracy: 0.4155443145522916\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118439.40537273619\n",
      "Current Training Accuracy: 0.4156615321679752\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118417.96318710737\n",
      "Current Training Accuracy: 0.41565681199553156\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118386.57192575755\n",
      "Current Training Accuracy: 0.41571896093270605\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118366.0896014429\n",
      "Current Training Accuracy: 0.415822804726466\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118342.18668834133\n",
      "Current Training Accuracy: 0.41598643737117863\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118302.39620812204\n",
      "Current Training Accuracy: 0.41629482197082934\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118283.44976814429\n",
      "Current Training Accuracy: 0.4165025095583492\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118258.68406338841\n",
      "Current Training Accuracy: 0.4165032962537565\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118237.54872666797\n",
      "Current Training Accuracy: 0.4165103765124219\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118215.13318089176\n",
      "Current Training Accuracy: 0.41660556665670184\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118186.95909938731\n",
      "Current Training Accuracy: 0.41679909372689083\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118158.94671552486\n",
      "Current Training Accuracy: 0.4169776735843416\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118132.30969499843\n",
      "Current Training Accuracy: 0.4171153452806142\n",
      "-------------------------------------------------------------------\n",
      "Current Cost: 118109.19154275884\n",
      "Current Training Accuracy: 0.41722626933303963\n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "complete_res, complete_f, complete_d = sy.optimize.fmin_l_bfgs_b(cost, \n",
    "                                [0.0]*nItems + # Initialize beta\n",
    "                                [random.random() * 0.1 - 0.05 for k in range(K*(nUsers + nItems))], # Gamma\n",
    "                                derivative, args=[0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unpack(complete_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions, labels = generate_outputs(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions, labels = generate_outputs(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percision_user(user):\n",
    "    \"\"\"\n",
    "    returns the predicted ratings for the specified user,\n",
    "    this is mainly used in computing evaluation metric\n",
    "    \"\"\"\n",
    "    user_pred = []\n",
    "    copy_items = copy.deepcopy(itemsPerUser[user])\n",
    "    for i in itemsPerUser[user]:\n",
    "        copy_items.remove(i)\n",
    "        predict = sigmoid(prediction(user, i, random.choice(copy_items)))\n",
    "        copy_items.add(i)\n",
    "        user_pred.append(predict)\n",
    "        \n",
    "    return user_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percision_score():\n",
    "    score = 0.0\n",
    "    for user in users:\n",
    "        score += sum(np.rint(percision_user))\n",
    "    score =/ nUsers\n",
    "    return score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
